{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Blog!","text":""},{"location":"#about","title":"About","text":"<p>I will put my daily study notes, homework, and resources on this website. Not a decent blog anymore right xd</p>"},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/","title":"Concept Aware Deep Knowledge Tracing and ExerciseRecommendation in an Online Learning System","text":""},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#concept-aware-deep-knowledge-tracing-and-exerciserecommendation-in-an-online-learning-system","title":"Concept-Aware Deep Knowledge Tracing and ExerciseRecommendation in an Online Learning System","text":""},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#abstract","title":"Abstract","text":"<p>\u63d0\u51fa\u4e00\u79cd\u65b0\u7684TK\u2014\u2014Dynamic Key-Value Memory Network (DKVMN)</p> <p>\uff08\u5176\u5b9e\u4e5f\u662f\u4e00\u79cdexercise-concept \u7684 mapping)</p> <p>\u7528\u8fd9\u4e2aTK\u5efa\u7acbMDP \uff0c \u63a5\u7740\u4ed6\u4eec\u7528RL\u5728\u4e0a\u9762\u8fdb\u884cpolicy eval and policy improvement</p> <p>\u89c2\u770b\u672c\u7bc7\u6587\u7ae0\uff0c\u63a5\u4e0b\u6765\u7684\u76ee\u7684\u5c31\u53ea\u662f\u5355\u7eaf\u770b\u4ed6\u7684TK\u8bbe\u8ba1\u4e86</p>"},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#1introduction","title":"1.Introduction","text":"<p>\u4f5c\u8005\u8bbe\u8ba1\u4e86\u7684system\u76f8\u6bd4\u4e8eMOOCs\uff0c \u91cd\u70b9\u5f3a\u8c03\u4e86\u4ee5\u4e0b\u4e24\u4e2a\u7279\u70b9</p> <p>\u5176\u5b9e\u6ca1\u5dee</p> <p></p> <p></p> <p>\u77e5\u8bc6\u70b9\u7684\u6982\u5ff5\u7279\u5f81  \u6682\u65f6\u4e0d\u77e5\u9053\u662f\u5565</p> <p></p> <p></p>"},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#2-related-work","title":"2. Related work","text":"<p>DTK\u548cBTK\u90fd\u57fa\u672c\u63d0\u5230\u4e86\uff08\u5305\u62ec\u540e\u7eed\u6709\u4eba\u63d0\u51fa\u7684\u6539\u8fdb\uff09  \u4e0d\u8fc7\u6ca1\u63d0\u5230ACT </p>"},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#3-background","title":"3. Background","text":""},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#31-intelligent-practice-systemips","title":"3.1 Intelligent Practice System(IPS)","text":"<p>\u8fd9\u4e2a\u5c31\u662f\u4e00\u4e2a\u7ebf\u4e0aOJ\u7cfb\u7edf \u5206\u4e3a1 - 7\u4e2astages \u6bcf\u4e2astages \u90fd\u5305\u62ec\u4e00\u4e2aknowledge\u3002</p> <p>\u6bcf\u4e2aknowledge \u90fd\u5305\u6db5\u4e09\u4e2a tags    \u8fd9\u5c31\u89e3\u91ca\u4e86\u4e0a\u9762\u7684\u77e5\u8bc6\u70b9\u6982\u5ff5\u7279\u5f81(exercises's knowledge concept properties)</p> <p>\u6211\u4e5f\u540c\u65f6\u627e\u5230\u4e86DKVMN\u7684\u6587\u732e</p> <p></p> <p>\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u7684\u6a21\u578b\u7ed3\u5408\u4e86BKT\u548cDKT\u7684\u4f18\u70b9\uff1a\u5f00\u53d1\u6982\u5ff5\u4e4b\u95f4\u5173\u7cfb\u7684\u80fd\u529b\u548c\u8ddf\u8e2a\u6bcf\u4e2a\u6982\u5ff5\u72b6\u6001\u7684\u80fd\u529b</p> <p>\u7ec6\u8282\u5728\u4e0b\u9762\u8bb2...</p>"},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#32-data-set-and-data-pre-processing","title":"3.2 Data Set and Data Pre-Processing","text":""},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#4-knowledge-tracing-model","title":"4. Knowledge Tracing Model","text":""},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#41-concept-aware-memory-structure","title":"4.1 Concept-Aware Memory Structure","text":"<p>\u4f5c\u8005\u4fee\u6539 DKVMN \u4ee5\u6839\u636e\u8bfe\u7a0b\u7684\u6982\u5ff5\u5217\u8868\u8bbe\u8ba1\u5176\u5185\u5b58\u7ed3\u6784</p> <p>\u4e3b\u8981\u662f\u8fd9\u5f20\u56fe\uff1a</p> <p></p> <p>\u901a\u8fc7\u8f93\u5165\u95ee\u9898qt \u7684\u5230\u6700\u540e\u77e5\u8bc6\u6982\u5ff5\u6743\u91cd</p> <p>\uff08\u5177\u4f53\u662f\u600e\u4e48\u5f97\u5230\u7684\uff1f\u56de\u5934\u770b\u8bba\u6587\u5427\uff09</p>"},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#43-read-process","title":"4.3 Read Process","text":"<p>\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u83b7\u5f97\u7684 KCW \u6765\u8ba1\u7b97\u7528\u6237\u5f53\u524d\u77e5\u8bc6\u6982\u5ff5\u72b6\u6001\u7684\u52a0\u6743\u548c\uff0c\u4ee5\u9884\u6d4b\u5b66\u751f\u5728\u7ec3\u4e60\u4e2d\u7684\u8868\u73b0 rt</p> <p>\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06 rt \u4e0e\u7ec3\u4e60\u7684\u96be\u5ea6\u548c\u9636\u6bb5\u7279\u5f81\u7684\u5d4c\u5165\u8fde\u63a5\u8d77\u6765\uff0c\u5373 dt \u548c gt\u3002\u7ed3\u679c\u518d\u7ecf\u8fc7\u4e00\u4e2a\u5e26\u6709\u6fc0\u6d3b\u51fd\u6570Tanh\u7684\u5168\u8fde\u63a5\u5c42\u5f97\u5230\u4e00\u4e2a\u6c47\u603b\u5411\u91cfft\uff0c\u5b83\u5305\u542b\u4e86\u5b66\u751f\u4e0eqt\u76f8\u5173\u7684\u77e5\u8bc6\u72b6\u6001\u7684\u6240\u6709\u4fe1\u606f\u548c\u7ec3\u4e60\u7684\u7279\u5f81</p> <p></p> <p>\u6700\u540e\uff0cft\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u8f93\u51fa\u5b66\u751f\u6b63\u786e\u5b8c\u6210\u7ec3\u4e60qt\u7684\u6982\u7387\u3002\u7528 p \u8868\u793a\u6982\u7387</p> <p></p>"},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#44-update-process","title":"4.4 Update Process","text":"<p>\u66f4\u65b0\u8fd9\u4e00\u8fc7\u7a0b</p> <p></p> <p>\u4f5c\u8005\u63d0\u51fa\uff0c\u76f8\u6bd4\u4e8eDKVMN \u4ed6\u4eec\u8fd8\u5c06\u5b66\u751f\u505a\u9898\u65f6\u95f4\u8003\u8651\u8fdb\u53bb\u4e86\u3002\u4f1a\u5728\u5b66\u751f\u505a\u5b8c\u9898\u76ee\u540e\uff0c\u5c06\u65f6\u95f4\u548c\u6b63\u786e\u7387\u90fd\u4f5c\u4e3a\u66f4\u65b0\u53c2\u6570\u3002</p> <p>\u5176\u4ed6\u7684\u66f4\u65b0\u548cDKVMN\u7c7b\u4f3c</p>"},{"location":"Concept-Aware%20Deep%20Knowledge%20Tracing%20and%20ExerciseRecommendation%20in%20an%20Online%20Learning%20System/#5-exercise-recommendation-based-on-reinforcement-learning","title":"5. Exercise Recommendation Based on Reinforcement Learning","text":"<p>\u8fd9\u91cc\u4f7f\u7528\u4e86POMDP\uff08\u90e8\u5206\u89c2\u6d4b\uff09</p> <p>state: \u5b66\u751f\u7684\u6f5c\u5728\u77e5\u8bc6\u70b9\u638c\u63e1\u60c5\u51b5</p> <p>action\uff1a\u9898\u76ee</p> <p>agent:  \u63a8\u8350\u7b56\u7565</p> <p>env: \u5b66\u751f</p> <p>\u5176\u4ed6\u6211\u770b\u4e0d\u61c2\u4e8655 \u6682\u65f6\u8fd8\u6ca1\u770b\u8fc7POMDP\u7684\u51b3\u7b56\u8fc7\u7a0b\u548c\u51b3\u7b56\u7b97\u6cd5</p>"},{"location":"Dynamic%20Programming/","title":"Dynamic Programming","text":"<p>Richard Bellman</p> <p>2023/2/27 \u6ce8\u91ca</p>"},{"location":"Dynamic%20Programming/#background","title":"Background","text":"<ul> <li>\u4eba\u7c7b\u5728\u53d1\u5c55\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u65ad\u63d0\u9ad8\u81ea\u5df1\u601d\u8003\u95ee\u9898\uff0c\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002</li> </ul> <p>Man is a decision-making animal and, above all, conscious of this fact. This self-scrutiny has led to continuing efforts to find efficient ways of behaving in the face of complexity and uncertainty.</p>"},{"location":"Dynamic%20Programming/#feedback-control","title":"Feedback Control","text":"<p>\u4f5c\u8005\u8ba9\u6211\u4eec\u8003\u8651\u8fd9\u6837\u4e00\u4e2a\u60c5\u51b5\uff1a\u98de\u8239\u964d\u843d\u6708\u7403\u7684\u573a\u666f</p> <p></p> <p>\u76ee\u6807\u5c31\u662f\u6700\u77ed\u65f6\u95f4\u6700\u5c11\u529f\u8017...\u5230\u8fbe\u76ee\u7684\u5730</p> <p>\u90a3\u4e48\u8fd9\u4e2a\u95ee\u9898\u6709\u65f6\u5019\u5f88\u5bb9\u6613\u88ab\u6211\u4eec\u7406\u60f3\u5316\u4e86\uff0c\u5bf9\u6b64\u6211\u4eec\u4e0d\u5f97\u4e0d\u968f\u65f6\u5bf9\u8fd9\u6837\u7684trajectory\u8fdb\u884c\u76d1\u63a7\uff0c\u7531\u4e8e\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u6709\u65f6\u5019\u6211\u4eec\u7684\u8def\u5f84\u53ef\u80fd\u53d8\u6210\u8fd9\u6837\uff1a</p> <p></p> <p>\u611a\u8822\u7684\u529e\u6cd5\u5c31\u662f\u5c06\u8fd9\u4e2a\u8def\u5f84\u4e0d\u505c\u7684\u6839\u636e\u201c\u521a\u521a\u5236\u5b9a\u7684\u8def\u5f84\u201d\u4e0d\u65ad\u7684\u4fee\u6539\u65b9\u6848\uff0c\u4f7f\u5f97\u6211\u4eec\u7684\u8def\u5f84\u56de\u5f52\u5230\u539f\u59cb\u72b6\u6001\u3002\u4f46\u662f\u8fd9\u4e2a\u663e\u7136\u592a\u611a\u8822\u4e86\u3002\u52a8\u6001\u89c4\u5212\uff0c\u6700base\u7684\u5730\u65b9\u5c31\u662f\u5343\u4e07\u4e0d\u80fd\u5fd8\u8bb0\u6700\u4e3b\u8981\u7684\u76ee\u7684\u3002</p> <p></p>"},{"location":"Dynamic%20Programming/#policy-concept","title":"Policy Concept","text":"<p>\u591a\u9636\u6bb5\u51b3\u7b56\u88ab\u89c6\u4e3a\u653f\u7b56\u7684\u91cd\u590d\u5e94\u7528\u3002\u5728\u6700\u5c0f\u5316\u65f6\u95f4\u3001\u71c3\u6599\u3001\u6210\u672c\u6216\u6700\u5927\u5316\u5229\u6da6\u7684\u610f\u4e49\u4e0a\u6700\u6709\u6548\u7684\u7b56\u7565\u79f0\u4e3a\u6700\u4f18\u7b56\u7565\u3002</p> <p>\u57fa\u4e8e\u521a\u521a\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5e94\u8be5\u60f3\u5230\u7684\u662f\u8fd9\u6837\u7684\u7b56\u7565\uff1a</p> <p></p> <p>\u8fd9\u662fDP\u4e2d\u6700\u6838\u5fc3\u7684\u4e00\u70b9\u3002</p> <p>\u601d\u8003\u5e76\u4f7f\u7528DP\u89e3\u51b3\u95ee\u9898\u7684\u672c\u8d28\u5c31\u662f\u8981\u65f6\u523b\u5173\u6ce8\u5f53\u524d\u72b6\u6001\uff0c\u65e0\u8bba\u4e4b\u524d\u6709\u591a\u5c11\u675f\u7f1a\uff0c\u6211\u4eec\u5728\u6b64\u523b\u90fd\u662f\u8981\u6700\u4f18\u5316\u73b0\u5904\u7684\u7b56\u7565\u3002</p> <p>\u52a8\u6001\u89c4\u5212\u662f\u4e00\u79cd\u6570\u5b66\u6280\u672f\uff0c\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u51b3\u7b56\u5236\u5b9a\uff0c\u5176\u5e94\u7528\u4e8e\u9884\u6d4b\u548c\u63a7\u5236\u95ee\u9898</p>"},{"location":"Dynamic%20Programming/#application","title":"Application","text":"<p>\u8fd9\u91cc\u4f5c\u8005\u4e3b\u8981\u60f3\u8bb2\u8ff0\u5f53\u65f6\u5e74\u4ee3\uff08\u6b64\u8bba\u6587\u5199\u4e8e1966\u5e74\uff09\uff0c\u8feb\u4e8e\u8ba1\u7b97\u673a\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0c\u52a8\u6001\u89c4\u5212\u95ee\u9898\u4e00\u76f4\u90fd\u4e0d\u592a\u80fd\u591f\u5b9e\u73b0\u5927\u90e8\u5206\u7684Application\uff0c\u4f46\u968f\u7740\u6280\u672f\u8fdb\u6b65\uff0c\u52a8\u6001\u89c4\u5212\u53d8\u5f97\u66f4\u52a0\u5e7f\u6cdb\u9002\u7528\u3002</p>"},{"location":"Dynamic%20Programming/#multistage-decision-making","title":"Multistage Decision Making","text":"<p>\u8fd9\u6bb5\u8bdd\u5f88\u9707\u64bc</p> <p>\u4f5c\u8005\u5728\u8fd9\u6bb5\u60f3\u8868\u793a\uff0c\u5f53\u6211\u4eec\u5982\u679c\u9762\u4e34\u7684\u95ee\u9898\u662f\u4e00\u4e2a\u590d\u6742\u51b3\u7b56\u4e2d\u6d89\u53ca\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5e94\u8be5\u4e5f\u7ee7\u7eed\u60f3\u5230DP\u7ed9\u6211\u4eec\u5e26\u6765\u7684\u7b97\u6cd5\u597d\u5904\u3002\u9996\u5148\uff0c\u5bf9\u4e8e\u8fd9\u6837\u7684\u95ee\u9898\uff0c\u6211\u4eec\u53ef\u4ee5\u7b80\u5316\u5047\u8bbe\uff0c\u4f8b\u5982\uff1a\u6211\u4eec\u53ef\u4ee5\u51c6\u786e\u786e\u5b9a\u7cfb\u7edf\u5728\u4efb\u4f55\u65f6\u5019\u7684\u72b6\u6001\uff0c\u5e76\u4e14\u77e5\u9053\u4f55\u65f6\u65bd\u52a0\u63a7\u5236\u4ee5\u53ca\u5176\u6548\u679c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5047\u8bbe\u90fd\u662f\u7406\u60f3\u5316\u7684\uff0c\u5f53\u5b83\u4eec\u5728\u79d1\u5b66\u4e0a\u9650\u5236\u4e86\u6211\u4eec\u7406\u89e3\u548c\u9884\u6d4b\u7684\u80fd\u529b\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u51c6\u5907\u4fee\u6539\u5b83\u4eec\u3002</p> <p>\u5728\u8fd9\u91cc\uff0c\u4f5c\u8005\u4ec5\u8003\u8651\u4e86\u56e0\u679c\u9884\u6d4b\u9700\u8981\u4fee\u6539\u7684\u60c5\u51b5\u3002\u5982\u679c\u6211\u4eec\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u51b3\u7b56\u7684\u6548\u679c\uff0c\u90a3\u4e48\u6709\u6548\u51b3\u7b56\u7684\u95ee\u9898\u5c31\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\u3002\u6b64\u65f6\uff0c\u201c\u6709\u6548\u201d\u6216\u201c\u6700\u4f18\u201d\u7684\u5b9a\u4e49\u4e5f\u53d8\u5f97\u6a21\u7cca\u3002\u6982\u7387\u7406\u8bba\u662f\u4e00\u79cd\u7814\u7a76\u4e0d\u53ef\u9884\u6d4b\u6548\u679c\u7684\u6570\u5b66\u7406\u8bba\uff0c\u4f46\u8fd9\u4e2a\u7406\u8bba\u5e76\u4e0d\u80fd\u6db5\u76d6\u6240\u6709\u7c7b\u578b\u7684\u968f\u673a\u4e8b\u4ef6\uff0c\u53ea\u80fd\u5904\u7406\u7279\u5b9a\u7c7b\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u3002</p> <p>\u5728\u8fd9\u91cc\uff0c\"\u6709\u6548\"\uff08effective\uff09\u548c\"\u6700\u4f18\"\uff08optimal\uff09\u662f\u6307\u5728\u9762\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u5b9e\u73b0\u76ee\u6807\u7684\u51b3\u7b56\u7b56\u7565\u3002\u8fd9\u4e9b\u7b56\u7565\u9700\u8981\u8003\u8651\u5230\u53ef\u80fd\u7684\u968f\u673a\u4e8b\u4ef6\uff0c\u4ee5\u4fbf\u5728\u591a\u4e2a\u9636\u6bb5\u7684\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u4f7f\u6211\u4eec\u5c3d\u53ef\u80fd\u63a5\u8fd1\u76ee\u6807\u3002\u6709\u6548\u548c\u6700\u4f18\u7b56\u7565\u53ef\u4ee5\u6700\u5927\u5316\u6536\u76ca\u3001\u6700\u5c0f\u5316\u635f\u5931\u6216\u5b9e\u73b0\u5176\u4ed6\u7279\u5b9a\u7684\u76ee\u6807\u3002</p> <p>\u5728\u524d\u9762\u7684\u63b7\u786c\u5e01\u8d4c\u535a\u6e38\u620f\u793a\u4f8b\u4e2d\uff0c\"\u6709\u6548\"\u548c\"\u6700\u4f18\"\u7b56\u7565\u662f\u6307\u5728\u6bcf\u6b21\u629b\u786c\u5e01\u65f6\uff0c\u6307\u5bfc\u4f60\u4e0b\u6ce8\u91d1\u989d\u7684\u89c4\u5219\uff0c\u4f7f\u4f60\u80fd\u591f\u572810\u8f6e\u6e38\u620f\u4e2d\u5b9e\u73b0\u5c06100\u5143\u589e\u52a0\u5230200\u5143\u7684\u76ee\u6807\u3002\u8fd9\u4e2a\u7b56\u7565\u9700\u8981\u5728\u6bcf\u8f6e\u6e38\u620f\u4e2d\u90fd\u80fd\u7ed9\u51fa\u6700\u4f73\u7684\u4e0b\u6ce8\u5efa\u8bae\uff0c\u4ee5\u4fbf\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u4f7f\u4f60\u7684\u6536\u76ca\u6700\u5927\u5316\u3002</p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u9762\u5bf9\u4e0d\u786e\u5b9a\u6027\u65f6\uff0c\"\u6709\u6548\"\u548c\"\u6700\u4f18\"\u5e76\u4e0d\u610f\u5473\u7740\u6211\u4eec\u53ef\u4ee5\u5b8c\u5168\u9884\u6d4b\u51b3\u7b56\u7684\u7ed3\u679c\uff0c\u800c\u662f\u6211\u4eec\u5728\u53ef\u7528\u4fe1\u606f\u548c\u5f53\u524d\u60c5\u51b5\u4e0b\uff0c\u91c7\u53d6\u4e86\u6700\u5408\u9002\u7684\u884c\u52a8\u3002\u8fd9\u4e9b\u7b56\u7565\u65e8\u5728\u4f7f\u6211\u4eec\u5728\u6574\u4e2a\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5c3d\u53ef\u80fd\u5730\u5b9e\u73b0\u76ee\u6807\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u5173\u6ce8\u5355\u4e2a\u9636\u6bb5\u7684\u7ed3\u679c\u3002</p> <p>\u4f5c\u8005\u63d0\u5230\uff0c\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u7814\u7a76\u51b3\u7b56\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u5c06\u95ee\u9898\u9886\u57df\u8f6c\u5411\u8d4c\u535a\u7cfb\u7edf\u3002\u8d4c\u535a\u7cfb\u7edf\u6d89\u53ca\u7684\u6570\u5b66\u539f\u7406\u4e0e\u4fdd\u9669\u516c\u53f8\u7684\u7cbe\u7b97\u3001\u534e\u5c14\u8857\u7684\u6295\u8d44\u8ba1\u5212\u4ee5\u53ca\u53ef\u9760\u6027\u7406\u8bba\u3001\u5e93\u5b58\u7406\u8bba\u7b49\u65b9\u9762\u5177\u6709\u76f8\u540c\u7684\u62bd\u8c61\u6027\u8d28\u3002</p> <p>\u8d4c\u535a\u7cfb\u7edf\u9700\u8981\u4e00\u79cd\u653f\u7b56\uff0c\u8fd9\u4e2a\u653f\u7b56\u544a\u8bc9\u8d4c\u5f92\u5728\u6bcf\u79cd\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\u5e94\u8be5\u505a\u51fa\u4ec0\u4e48\u51b3\u7b56\u3002\u52a8\u6001\u89c4\u5212\u5df2\u7ecf\u572821\u70b9\u7b49\u8d4c\u535a\u6e38\u620f\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\u3002\u5728\u6d89\u53ca\u968f\u673a\u4e8b\u4ef6\u7684\u591a\u9636\u6bb5\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u7075\u6d3b\u7684\u7b56\u7565\u975e\u5e38\u9002\u7528\u3002\u6700\u4f18\u6027\u539f\u7406\u4e3a\u5904\u7406\u786e\u5b9a\u6027\u548c\u6982\u7387\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u5de5\u5177\u3002\u968f\u673a\u8fd9\u4e2a\u672f\u8bed\u7ecf\u5e38\u7528\u6765\u63cf\u8ff0\u6d89\u53ca\u968f\u673a\u4e8b\u4ef6\u7684\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u5b83\u6ca1\u6709\u65e5\u5e38\u751f\u6d3b\u7684\u542b\u4e49\u3002</p> <p>\u4e3e\u4e2a\u4f8b\u5b50:</p> <p>\u5047\u8bbe\u4f60\u5728\u4e00\u573a\u63b7\u786c\u5e01\u6e38\u620f\u4e2d\uff0c\u6bcf\u6b21\u629b\u786c\u5e01\uff0c\u6b63\u9762\u671d\u4e0a\u4f60\u8d62\uff0c\u53cd\u9762\u671d\u4e0a\u4f60\u8f93\u3002\u4f60\u5f00\u59cb\u65f6\u6709100\u5143\uff0c\u4f60\u7684\u76ee\u6807\u662f\u901a\u8fc7\u5408\u7406\u7684\u4e0b\u6ce8\u7b56\u7565\uff0c\u572810\u8f6e\u6e38\u620f\u4e2d\u8d62\u5f97200\u5143\u3002\u5728\u8fd9\u4e2a\u573a\u666f\u4e2d\uff0c\u4f60\u9762\u4e34\u7684\u95ee\u9898\u5c31\u662f\u5982\u4f55\u5728\u6bcf\u6b21\u629b\u786c\u5e01\u65f6\u51b3\u5b9a\u4e0b\u6ce8\u591a\u5c11\u94b1\uff0c\u4ee5\u5b9e\u73b0\u4f60\u7684\u76ee\u6807\u3002</p> <p>\u5728\u8fd9\u4e2a\u6e38\u620f\u4e2d\uff0c\u6709\u8bb8\u591a\u4e0d\u786e\u5b9a\u56e0\u7d20\uff0c\u6bd4\u5982\u6bcf\u6b21\u629b\u786c\u5e01\u7684\u7ed3\u679c\u662f\u968f\u673a\u7684\u3002\u7531\u4e8e\u8fd9\u4e2a\u4e0d\u786e\u5b9a\u6027\uff0c\u6211\u4eec\u65e0\u6cd5\u786e\u5207\u5730\u9884\u6d4b\u6bcf\u6b21\u51b3\u7b56\u7684\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u8fd0\u7528\u6982\u7387\u7406\u8bba\u6765\u5e2e\u52a9\u6211\u4eec\u8fdb\u884c\u51b3\u7b56\u3002</p> <p>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f60\u53ef\u4ee5\u5236\u5b9a\u4e00\u4e2a\u7b56\u7565\uff0c\u6839\u636e\u4f60\u5f53\u524d\u62e5\u6709\u7684\u91d1\u989d\u4ee5\u53ca\u4f60\u7684\u76ee\u6807\uff0c\u544a\u8bc9\u4f60\u6bcf\u6b21\u5e94\u8be5\u4e0b\u6ce8\u591a\u5c11\u3002\u8fd9\u4e2a\u7b56\u7565\u9700\u8981\u5728\u6bcf\u8f6e\u6e38\u620f\u4e2d\u90fd\u80fd\u6307\u5bfc\u4f60\u505a\u51fa\u51b3\u7b56\uff0c\u540c\u65f6\u8003\u8651\u5230\u6e38\u620f\u7684\u968f\u673a\u6027\u3002\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u4f60\u627e\u5230\u8fd9\u6837\u4e00\u4e2a\u7b56\u7565\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u5728\u6bcf\u4e2a\u9636\u6bb5\u90fd\u4f18\u5316\u4f60\u7684\u51b3\u7b56\uff0c\u4f7f\u4f60\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u8fbe\u5230\u6700\u4f18\u7ed3\u679c\u3002</p> <p>\u603b\u4e4b\uff0c\u8fd9\u6bb5\u6587\u5b57\u4e3b\u8981\u8ba8\u8bba\u4e86\u5728\u9762\u5bf9\u4e0d\u786e\u5b9a\u6027\u65f6\u5982\u4f55\u8fdb\u884c\u590d\u6742\u51b3\u7b56\u3002\u901a\u8fc7\u8fd0\u7528\u6982\u7387\u7406\u8bba\u548c\u52a8\u6001\u89c4\u5212\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u968f\u673a\u4e8b\u4ef6\u4e2d\u627e\u5230\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\u6765\u6307\u5bfc\u6211\u4eec\u7684\u51b3\u7b56\u3002</p> <p>\u604d\u7136\u5927\u609f\uff0c\u52a8\u6001\u89c4\u5212\u89e3\u51b3MDP\u7684\u5f71\u5b50</p>"},{"location":"Dynamic%20Programming/#adaptive-control","title":"Adaptive Control","text":"<p>\u4f20\u7edf\u7684\u51b3\u7b56\u8fc7\u7a0b\u6211\u4eec\u662f\u5728\u5df2\u77e5\u60c5\u51b5\u4e0b\u505a\u51fa\u5047\u8bbe\uff1a</p> <p></p> <p>\u4f46\u662f\u4f5c\u8005\u63d0\u51fa\uff0c\u5728\u5f88\u591a\u91cd\u8981\u5de5\u4f5c\u4e2d\uff08\u5b9e\u9645\u4e0a\u5728\u6240\u6709\u51b3\u7b56\u7684\u573a\u666f\u4e2d\uff09\uff0c\u6211\u4eec\u9700\u8981\u5728\u4e0d\u5b8c\u5168\u4e86\u89e3\u5e95\u5c42\u7cfb\u7edf\u7684\u57fa\u672c\u5de5\u4f5c\u539f\u7406\u7684\u60c5\u51b5\u4e0b\u505a\u51b3\u7b56\u3002</p> <p>\u8fd9\u4e2a\u65f6\u5019\u5c31\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u2014\u2014\u201cAdaptive Control\u201d\u81ea\u9002\u5e94\u63a7\u5236</p> <p>\u81ea\u9002\u5e94\u63a7\u5236\u662f\u4e00\u79cd\u57fa\u4e8e\u7ecf\u9a8c\u4fee\u6539\u6700\u4f18\u7b56\u7565\u7684\u65b9\u6cd5\u3002\u5728\u8fd9\u79cd\u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u4ece\u5bf9\u6700\u4f18\u7b56\u7565\u7684\u67d0\u4e9b\u9884\u8bbe\u6982\u5ff5\u5f00\u59cb\uff0c\u7136\u540e\u6839\u636e\u7ecf\u9a8c\u7cfb\u7edf\u5730\u4fee\u6539\u8fd9\u4e9b\u7b56\u7565\u3002\u81ea\u9002\u5e94\u63a7\u5236\u4e0e\u5fc3\u7406\u5b66\u5bb6\u6240\u8bf4\u7684\u9002\u5e94\u6709\u5bc6\u5207\u8054\u7cfb\u3002</p> <p>\u4e3e\u4e2a\u4f8b\u5b50\uff1a</p> <p>\u5728\u65e0\u4eba\u9a7e\u9a76\u6c7d\u8f66\u7684\u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u65e0\u6cd5\u63d0\u524d\u77e5\u9053\u6240\u6709\u53ef\u80fd\u9047\u5230\u7684\u9053\u8def\u3001\u4ea4\u901a\u548c\u5929\u6c14\u6761\u4ef6\u3002\u540c\u65f6\uff0c\u65e0\u4eba\u9a7e\u9a76\u6c7d\u8f66\u9700\u8981\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u4e2d\u505a\u51fa\u5b9e\u65f6\u51b3\u7b56\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u6839\u636e\u5b9e\u65f6\u6570\u636e\u8c03\u6574\u5176\u884c\u4e3a\u7684\u81ea\u9002\u5e94\u63a7\u5236\u7cfb\u7edf\u3002</p> <p>\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u81ea\u9002\u5e94\u63a7\u5236\u7cfb\u7edf\u9996\u5148\u57fa\u4e8e\u9884\u5148\u8bbe\u5b9a\u7684\u4e00\u7ec4\u89c4\u5219\uff08\u4f8b\u5982\u9075\u5b88\u4ea4\u901a\u6cd5\u89c4\u3001\u4fdd\u6301\u5b89\u5168\u8ddd\u79bb\u7b49\uff09\uff0c\u5728\u5b9e\u9645\u9a7e\u9a76\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u6536\u96c6\u5230\u5404\u79cd\u6570\u636e\uff0c\u5982\u5176\u4ed6\u8f66\u8f86\u7684\u4f4d\u7f6e\u3001\u901f\u5ea6\uff0c\u9053\u8def\u72b6\u51b5\u7b49\u3002\u57fa\u4e8e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u7cfb\u7edf\u4f1a\u4e0d\u65ad\u8c03\u6574\u81ea\u5df1\u7684\u884c\u4e3a\uff0c\u4ee5\u9002\u5e94\u5f53\u524d\u7684\u73af\u5883\u3002</p> <p>\u4f8b\u5982\uff0c\u5f53\u65e0\u4eba\u9a7e\u9a76\u6c7d\u8f66\u9047\u5230\u62e5\u5835\u65f6\uff0c\u81ea\u9002\u5e94\u63a7\u5236\u7cfb\u7edf\u53ef\u80fd\u4f1a\u6839\u636e\u5b9e\u65f6\u4ea4\u901a\u6570\u636e\u5bfb\u627e\u6700\u4f73\u8def\u7ebf\u3002\u5982\u679c\u9047\u5230\u7a81\u5982\u5176\u6765\u7684\u6076\u52a3\u5929\u6c14\uff0c\u7cfb\u7edf\u53ef\u80fd\u4f1a\u964d\u4f4e\u8f66\u901f\u4ee5\u4fdd\u8bc1\u884c\u9a76\u5b89\u5168\u3002\u5728\u6240\u6709\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0c\u7cfb\u7edf\u90fd\u5728\u4e0d\u65ad\u5730\u5b66\u4e60\u5e76\u6839\u636e\u5b9e\u65f6\u6570\u636e\u8c03\u6574\u81ea\u5df1\u7684\u884c\u4e3a\u3002</p> <p>\u8fd9\u4e2a\u8fc7\u7a0b\u4e0e\u5fc3\u7406\u5b66\u4e2d\u7684\u9002\u5e94\u6982\u5ff5\u76f8\u4f3c\u3002\u65e0\u4eba\u9a7e\u9a76\u6c7d\u8f66\u9700\u8981\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u4e2d\u901a\u8fc7\u5b9e\u65f6\u5b66\u4e60\u548c\u8c03\u6574\uff0c\u4ee5\u786e\u4fdd\u5176\u51b3\u7b56\u59cb\u7ec8\u80fd\u591f\u6ee1\u8db3\u5b89\u5168\u3001\u6709\u6548\u548c\u8212\u9002\u7b49\u76ee\u6807\u3002</p>"},{"location":"Dynamic%20Programming/#hierarchy-of-decision-making","title":"Hierarchy of Decision Making","text":"<p>\u4f5c\u8005\u5728\u8fd9\u91cc\u8ba8\u8bba\u5230\u4e86\u201cmachine can learning\u201d \uff1f</p> <p>\u5173\u4e8e\u673a\u5668\u80fd\u5426\u601d\u8003\u8fd9\u4e00\u95ee\u9898\u5f15\u53d1\u4e86\u5f88\u591a\u4e89\u8bae\uff0c\u4f5c\u8005\u5728\u8fd9\u4e2a\u95ee\u9898\u6240\u7406\u89e3\u7684\u610f\u4e49\u53d6\u51b3\u4e8e\u6211\u4eec\u5982\u4f55\u5b9a\u4e49\u201c\u673a\u5668\u201d\u3001\u201c\u601d\u8003\u201d\u4ee5\u53ca\u201c\u80fd\u201d\u3002\u5728\u8fd9\u91cc\uff0c\u201c\u673a\u5668\u201d\u662f\u6307\u73b0\u6709\u7684\u5546\u7528\u6570\u5b57\u8ba1\u7b97\u673a\u3002\u800c\u4e3a\u4e86\u5b9a\u4e49\u201c\u601d\u8003\u201d\uff0c\u6211\u4eec\u5c06\u5176\u4e0e\u51b3\u7b56\u5236\u5b9a\u8054\u7cfb\u8d77\u6765\uff0c\u5c06\u601d\u8003\u8fc7\u7a0b\u7684\u4e0d\u540c\u5c42\u6b21\u4e0e\u51b3\u7b56\u8fc7\u7a0b\u7684\u4e0d\u540c\u5c42\u6b21\u7b49\u540c\u8d77\u6765\u3002\u6839\u636e\u8fd9\u79cd\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u63a2\u8ba8\u6211\u4eec\u662f\u5426\u80fd\u7f16\u5199\u4e00\u4e2a\u5728\u6307\u5b9a\u65f6\u95f4\u5185\u6267\u884c\u7279\u5b9a\u51b3\u7b56\u8fc7\u7a0b\u7684\u8ba1\u7b97\u673a\u7a0b\u5e8f\u3002\u201c\u80fd\u201d\u5177\u6709\u4e0d\u540c\u7684\u542b\u4e49\uff0c\u53d6\u51b3\u4e8e\u6211\u4eec\u5141\u8bb8\u7684\u65f6\u95f4\u662f2\u5206\u949f\u30012\u5c0f\u65f6\u8fd8\u662f2\u5e74\uff0c\u6216\u8005\u4ec5\u8981\u6c42\u65f6\u95f4\u662f\u6709\u9650\u7684\uff0c\u5c3d\u7ba1\u4e0d\u53ef\u9884\u6d4b\u3002</p> <p>\u76ee\u524d\uff0c\u8ba1\u7b97\u673a\u53ef\u4ee5\u88ab\u7f16\u7a0b\u6765\u73a9\u56fd\u9645\u8c61\u68cb\u6216\u8df3\u68cb\uff0c\u4f46\u5b83\u8fd8\u4e0d\u80fd\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u6210\u4e3a\u56fd\u9645\u8c61\u68cb\u5927\u5e08\u3002\u5982\u679c\u80fd\u5b9e\u73b0\u8fd9\u79cd\u80fd\u529b\uff0c\u5c06\u662f\u81ea\u9002\u5e94\u8fc7\u7a0b\u7406\u8bba\u7684\u91cd\u5927\u7a81\u7834\u3002</p> <p>\u6309\u7167\u8fd9\u79cd\u601d\u8def\uff0c\u53ef\u4ee5\u5c06\u51b3\u7b56\u8fc7\u7a0b\u5206\u4e3a\u4e0d\u540c\u5c42\u6b21\u3002\u7b2c\u4e00\u5c42\u662f\u4e0a\u6587\u63d0\u5230\u7684\u786e\u5b9a\u6027\u6216\u968f\u673a\u7c7b\u578b\u7684\u8fc7\u7a0b\u3002\u7b2c\u4e8c\u5c42\u6d89\u53ca\u4e86\u89e3\u7cfb\u7edf\u7ed3\u6784\u7684\u8fc7\u7a0b\u3002\u5728\u6bcf\u4e2a\u9636\u6bb5\u90fd\u9700\u8981\u4e00\u4e2a\u5c40\u90e8\u7b56\u7565\u6765\u505a\u51b3\u7b56\uff0c\u800c\u5168\u5c40\u7b56\u7565\u5219\u6839\u636e\u7ecf\u9a8c\u4fee\u6539\u5c40\u90e8\u7b56\u7565\u3002\u8fd9\u5c31\u662f\u5173\u4e8e\u51b3\u7b56\u7684\u51b3\u7b56\u3002\u9009\u62e9\u5168\u5c40\u7b56\u7565\u662f\u5173\u4e8e\u5173\u4e8e\u51b3\u7b56\u7684\u51b3\u7b56\u7684\u51b3\u7b56\u3002\u6211\u4eec\u53ef\u4ee5\u7ee7\u7eed\u6309\u7167\u8fd9\u79cd\u65b9\u5f0f\u8fdb\u884c\u3002</p> <p>\u9996\u5148\uff0c\u5728\u7b2c\u4e00\u5c42\uff0c\u6211\u4eec\u8003\u8651\u786e\u5b9a\u6027\u6216\u968f\u673a\u7c7b\u578b\u7684\u8fc7\u7a0b\u3002\u8fd9\u4e9b\u8fc7\u7a0b\u901a\u5e38\u6d89\u53ca\u57fa\u4e8e\u5df2\u77e5\u4fe1\u606f\u505a\u51fa\u51b3\u7b56\u3002\u4f8b\u5982\uff0c\u5728\u7ed9\u5b9a\u7684\u73af\u5883\u4e0b\u627e\u5230\u6700\u4f73\u8def\u5f84\u3002</p> <p>\u63a5\u4e0b\u6765\uff0c\u5728\u7b2c\u4e8c\u5c42\uff0c\u6211\u4eec\u8003\u8651\u5173\u4e8e\u7cfb\u7edf\u7ed3\u6784\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002\u5728\u8fd9\u4e00\u5c42\uff0c\u6211\u4eec\u9700\u8981\u5728\u6bcf\u4e2a\u9636\u6bb5\u4f7f\u7528\u5c40\u90e8\u7b56\u7565\uff08\u57fa\u4e8e\u5f53\u524d\u72b6\u6001\u548c\u4fe1\u606f\u505a\u51fa\u51b3\u7b56\uff09\uff0c\u5e76\u6839\u636e\u7ecf\u9a8c\u4fee\u6539\u8fd9\u4e9b\u5c40\u90e8\u7b56\u7565\u3002\u8fd9\u53ef\u4ee5\u79f0\u4e3a\u5173\u4e8e\u51b3\u7b56\u7684\u51b3\u7b56\uff0c\u56e0\u4e3a\u6211\u4eec\u5728\u8003\u8651\u5982\u4f55\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u8c03\u6574\u51b3\u7b56\u65b9\u6cd5\u3002\u8fd9\u4e00\u5c42\u7684\u5173\u952e\u662f\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u4ee5\u6539\u8fdb\u51b3\u7b56\u65b9\u6cd5\u3002</p> <p>(\u5173\u4e8e\u4ec0\u4e48\u662f\u7cfb\u7edf\u7ed3\u6784\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u6211\u4eec\u8bd5\u56fe\u4e86\u89e3\u548c\u5b66\u4e60\u6f5c\u5728\u7684\u7cfb\u7edf\u89c4\u5f8b\uff0cos:\u8bf4\u767d\u4e86\u5c31\u662fMDP\u4e2d\u7684env\u63d0\u4f9b\u7684observation\u6216\u8005reward\u7ed9\u6211\u4eec\u5e26\u6765\u7684\u4fe1\u606f\u3002</p> <p>\u7cfb\u7edf\u7ed3\u6784\u7684\u5b66\u4e60\u8fc7\u7a0b\u662f\u6307\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u8bd5\u56fe\u4e86\u89e3\u548c\u5b66\u4e60\u6f5c\u5728\u7684\u7cfb\u7edf\u89c4\u5f8b\u3001\u5173\u7cfb\u548c\u6a21\u5f0f\u3002\u8fd9\u4e9b\u89c4\u5f8b\u548c\u6a21\u5f0f\u6709\u52a9\u4e8e\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u7cfb\u7edf\u7684\u884c\u4e3a\uff0c\u4ece\u800c\u5728\u5c06\u6765\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002\u7b80\u800c\u8a00\u4e4b\uff0c\u5b66\u4e60\u7cfb\u7edf\u7ed3\u6784\u610f\u5473\u7740\u6211\u4eec\u8bd5\u56fe\u63ed\u793a\u7cfb\u7edf\u7684\u5185\u5728\u903b\u8f91\uff0c\u4ee5\u4fbf\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u52a0\u4ee5\u5229\u7528\u3002</p> <p>\u4f8b\u5982\uff0c\u5047\u8bbe\u4f60\u6b63\u5728\u7ba1\u7406\u4e00\u4e2a\u590d\u6742\u7684\u4f9b\u5e94\u94fe\u7cfb\u7edf\uff0c\u4f60\u9700\u8981\u51b3\u5b9a\u5982\u4f55\u4f18\u5316\u5e93\u5b58\u6c34\u5e73\u4ee5\u6ee1\u8db3\u5ba2\u6237\u9700\u6c42\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u4f60\u53ef\u80fd\u4f1a\u53d1\u73b0\u7cfb\u7edf\u4e2d\u7684\u67d0\u4e9b\u89c4\u5f8b\uff0c\u4f8b\u5982\u67d0\u4e9b\u4ea7\u54c1\u7684\u9700\u6c42\u5468\u671f\u6027\u6ce2\u52a8\u3001\u4f9b\u5e94\u5546\u7684\u4ea4\u8d27\u65f6\u95f4\u53ef\u9760\u6027\u7b49\u3002\u901a\u8fc7\u5b66\u4e60\u8fd9\u4e9b\u89c4\u5f8b\uff0c\u4f60\u53ef\u4ee5\u66f4\u597d\u5730\u9884\u6d4b\u672a\u6765\u7684\u9700\u6c42\u53d8\u5316\uff0c\u5e76\u76f8\u5e94\u5730\u8c03\u6574\u5e93\u5b58\u6c34\u5e73\u4ee5\u63d0\u9ad8\u6574\u4f53\u6548\u7387\u3002</p> <p>\u7cfb\u7edf\u7ed3\u6784\u7684\u5b66\u4e60\u8fc7\u7a0b\u901a\u5e38\u6d89\u53ca\u4ece\u7ecf\u9a8c\u548c\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u4ee5\u4fbf\u8c03\u6574\u548c\u4f18\u5316\u51b3\u7b56\u7b56\u7565\u3002\u8fd9\u53ef\u80fd\u5305\u62ec\u89c2\u5bdf\u7cfb\u7edf\u7684\u884c\u4e3a\u3001\u5206\u6790\u5386\u53f2\u6570\u636e\u3001\u8fdb\u884c\u8bd5\u9a8c\u7b49\u3002\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u901a\u8fc7\u8fd9\u4e9b\u5b66\u4e60\u8fc7\u7a0b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0d\u65ad\u66f4\u65b0\u548c\u6539\u8fdb\u6211\u4eec\u7684\u51b3\u7b56\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u7cfb\u7edf\u7684\u53d8\u5316\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u5c40\u90e8\u7b56\u7565\uff08\u57fa\u4e8e\u5f53\u524d\u72b6\u6001\u548c\u4fe1\u606f\u505a\u51fa\u7684\u51b3\u7b56\uff09\u53ef\u4ee5\u6839\u636e\u6240\u5b66\u5230\u7684\u7cfb\u7edf\u7ed3\u6784\u8fdb\u884c\u8c03\u6574\u548c\u4f18\u5316\u3002)</p> <p>\u518d\u5f80\u4e0a\uff0c\u9009\u62e9\u5168\u5c40\u7b56\u7565\uff08\u8c03\u6574\u5c40\u90e8\u7b56\u7565\u7684\u65b9\u6cd5\uff09\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5173\u4e8e\u5173\u4e8e\u51b3\u7b56\u7684\u51b3\u7b56\u7684\u51b3\u7b56\u3002\u8fd9\u610f\u5473\u7740\u6211\u4eec\u4e0d\u4ec5\u5728\u8c03\u6574\u5177\u4f53\u51b3\u7b56\uff0c\u8fd8\u5728\u8c03\u6574\u5982\u4f55\u8c03\u6574\u51b3\u7b56\u65b9\u6cd5\u672c\u8eab\u3002\u8fd9\u4e2a\u5c42\u6b21\u66f4\u52a0\u590d\u6742\uff0c\u56e0\u4e3a\u5b83\u6d89\u53ca\u5230\u5bf9\u6574\u4e2a\u51b3\u7b56\u8fc7\u7a0b\u7684\u4f18\u5316\u548c\u8c03\u6574\u3002</p> <p>\u603b\u7684\u6765\u8bf4\uff0c\u8fd9\u4e2a\u5c42\u6b21\u7ed3\u6784\u5f3a\u8c03\u4e86\u51b3\u7b56\u8fc7\u7a0b\u5728\u4e0d\u540c\u5c42\u6b21\u4e0a\u7684\u590d\u6742\u6027\u3002\u5728\u6bcf\u4e2a\u5c42\u6b21\u4e0a\uff0c\u6211\u4eec\u9700\u8981\u8003\u8651\u4e0d\u540c\u7684\u95ee\u9898\u548c\u7b56\u7565\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u5904\u7406\u5404\u79cd\u60c5\u51b5\u3002\u8fd9\u79cd\u5206\u5c42\u65b9\u6cd5\u6709\u52a9\u4e8e\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u51b3\u7b56\u8fc7\u7a0b\u7684\u4e0d\u540c\u65b9\u9762\uff0c\u5e76\u6709\u53ef\u80fd\u5f00\u53d1\u51fa\u66f4\u52a0\u667a\u80fd\u548c\u81ea\u9002\u5e94\u7684\u51b3\u7b56\u7cfb\u7edf\u3002</p> <p>\u65e0\u8bba\u6211\u4eec\u91c7\u7528\u54ea\u79cd\u65b9\u5f0f\u5f15\u5165\u5c42\u6b21\u7ed3\u6784\uff0c\u90fd\u4f1a\u9047\u5230\u4e00\u5b9a\u7684\u95ee\u9898\u3002\u4f8b\u5982\uff0c\u5728\u7279\u5b9a\u7684\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5982\u4f55\u786e\u5b9a\u5176\u5c42\u6b21\uff1f</p> <ol> <li>\u5728\u5f15\u5165\u5c42\u6b21\u7ed3\u6784\u65f6\uff0c\u6211\u4eec\u9700\u8981\u786e\u4fdd\u6bcf\u4e2a\u5c42\u6b21\u90fd\u5177\u6709\u660e\u786e\u7684\u76ee\u6807\u548c\u804c\u8d23\u3002\u8fd9\u5c06\u6709\u52a9\u4e8e\u786e\u4fdd\u5728\u6574\u4e2a\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u5404\u4e2a\u5c42\u6b21\u4e4b\u95f4\u7684\u6c9f\u901a\u548c\u534f\u4f5c\u5f97\u4ee5\u987a\u5229\u8fdb\u884c\u3002</li> <li>\u5728\u8bbe\u8ba1\u8fd9\u4e9b\u5c42\u6b21\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u8003\u8651\u5728\u4e0d\u540c\u5c42\u6b21\u4e4b\u95f4\u5206\u4eab\u4fe1\u606f\u548c\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002\u6709\u6548\u7684\u4fe1\u606f\u5171\u4eab\u53ef\u4ee5\u63d0\u9ad8\u51b3\u7b56\u8fc7\u7a0b\u7684\u6574\u4f53\u6548\u7387\uff0c\u5e76\u6709\u52a9\u4e8e\u907f\u514d\u91cd\u590d\u52aa\u529b\u548c\u8d44\u6e90\u6d6a\u8d39\u3002</li> <li>\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6211\u4eec\u53ef\u80fd\u9700\u8981\u6839\u636e\u7279\u5b9a\u95ee\u9898\u548c\u73af\u5883\u8c03\u6574\u5c42\u6b21\u7ed3\u6784\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u9700\u8981\u589e\u52a0\u66f4\u591a\u5c42\u6b21\u4ee5\u5904\u7406\u66f4\u590d\u6742\u7684\u95ee\u9898\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u9700\u8981\u51cf\u5c11\u5c42\u6b21\u4ee5\u7b80\u5316\u8fc7\u7a0b\u3002</li> <li>\u867d\u7136\u5c42\u6b21\u7ed3\u6784\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u548c\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u4f46\u5b83\u4e5f\u53ef\u80fd\u5e26\u6765\u4e00\u5b9a\u7a0b\u5ea6\u7684\u7ba1\u7406\u6311\u6218\u3002\u4f8b\u5982\uff0c\u53ef\u80fd\u9700\u8981\u66f4\u591a\u7684\u534f\u8c03\u548c\u6c9f\u901a\u6765\u786e\u4fdd\u5404\u4e2a\u5c42\u6b21\u4e4b\u95f4\u7684\u987a\u5229\u5408\u4f5c\u3002\u56e0\u6b64\uff0c\u5728\u5f15\u5165\u5c42\u6b21\u7ed3\u6784\u65f6\uff0c\u6211\u4eec\u9700\u8981\u6743\u8861\u5176\u4f18\u7f3a\u70b9\u3002</li> <li>\u5728\u6574\u4e2a\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u4e0d\u65ad\u5730\u8bc4\u4f30\u548c\u6539\u8fdb\u5c42\u6b21\u7ed3\u6784\u3002\u8fd9\u53ef\u80fd\u5305\u62ec\u5b9a\u671f\u5ba1\u67e5\u5404\u4e2a\u5c42\u6b21\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u6839\u636e\u9700\u8981\u8c03\u6574\u804c\u8d23\u548c\u76ee\u6807\u3002\u901a\u8fc7\u8fd9\u79cd\u6301\u7eed\u6539\u8fdb\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u786e\u4fdd\u5c42\u6b21\u7ed3\u6784\u59cb\u7ec8\u4fdd\u6301\u9002\u5e94\u6027\uff0c\u5e76\u80fd\u591f\u6709\u6548\u5730\u5e94\u5bf9\u65b0\u7684\u6311\u6218\u548c\u53d8\u5316\u3002</li> </ol>"},{"location":"Dynamic%20Programming/#conclusion","title":"Conclusion","text":"<p>\u4f5c\u8005\u8fd9\u6bb5\u8bdd\u5f15\u53d1\u7684\u601d\u8003\u5728\u5982\u4eca\u4e5f\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u7684\u95ee\u9898</p> <p>\u8fd9\u6bb5\u8bdd\u63d0\u5230\u4e86\u4eba\u7c7b\u5bf9\u5927\u8111\u8fc7\u7a0b\uff08\u5305\u62ec\u5fc3\u7406\u5b66\u548c\u751f\u7406\u5b66\u65b9\u9762\uff09\u7684\u7814\u7a76\u5174\u8da3\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u5230\u4e86\u6570\u5b57\u8ba1\u7b97\u673a\u7684\u63a8\u52a8\u3002\u5c3d\u7ba1\u76ee\u524d\u5c1a\u65e0\u5b8c\u6574\u7684\u7406\u8bba\u6765\u89e3\u91ca\u8fd9\u4e9b\u5f15\u4eba\u5165\u80dc\u7684\u95ee\u9898\uff0c\u4f46\u6570\u5b57\u8ba1\u7b97\u673a\u7684\u51fa\u73b0\u4fc3\u4f7f\u4eba\u4eec\u5bf9\u5927\u8111\u8fc7\u7a0b\u4ea7\u751f\u4e86\u6d53\u539a\u5174\u8da3\u3002</p> <p>\u4f5c\u8005\u6307\u51fa\uff0c\u5728\u5904\u7406\u4eba\u7c7b\u4e2a\u4f53\u548c\u793e\u4f1a\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u5b8c\u7f8e\u7684\u60c5\u51b5\u4e0b\u5e94\u5bf9\uff0c\u5982\u975e\u7406\u6027\u3001\u4e0d\u5408\u903b\u8f91\u3001\u4e0d\u4e00\u81f4\u548c\u4e0d\u5b8c\u6574\u7684\u884c\u4e3a\u3002\u7136\u800c\uff0c\u5728\u64cd\u4f5c\u8ba1\u7b97\u673a\u65f6\uff0c\u6211\u4eec\u5fc5\u987b\u6ee1\u8db3\u8be6\u7ec6\u7684\u6307\u4ee4\u548c\u7edd\u5bf9\u7cbe\u786e\u6027\u7684\u4e25\u683c\u8981\u6c42\u3002\u5982\u679c\u6211\u4eec\u7406\u89e3\u4e86\u4eba\u7c7b\u5728\u9762\u5bf9\u590d\u6742\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u7406\u6027\u65f6\u505a\u51fa\u6709\u6548\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u6bd4\u73b0\u5728\u66f4\u6709\u6548\u5730\u5229\u7528\u8ba1\u7b97\u673a\u3002</p> <p>\u795e\u7ecf\u7f51\u7edc\uff1f\u53ef\u89e3\u91ca\u6027\uff1f</p> <p>\u6b63\u662f\u5bf9\u8fd9\u4e00\u4e8b\u5b9e\u7684\u8ba4\u8bc6\u6fc0\u53d1\u4e86\u795e\u7ecf\u751f\u7406\u5b66\u9886\u57df\u7814\u7a76\u7684\u7e41\u8363\u3002\u8d8a\u6765\u8d8a\u591a\u5730\u7814\u7a76\u5927\u8111\u7684\u4fe1\u606f\u5904\u7406\u65b9\u9762\uff0c\u6211\u4eec\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u60d1\u548c\u60ca\u53f9\u3002\u5728\u5145\u5206\u7406\u89e3\u548c\u91cd\u73b0\u8fd9\u4e9b\u8fc7\u7a0b\u4e4b\u524d\uff0c\u6211\u4eec\u8fd8\u6709\u5f88\u957f\u7684\u8def\u8981\u8d70\u3002</p> <p>\u603b\u4e4b\uff0c\u6570\u5b66\u5bb6\u5728\u8bb8\u591a\u65b0\u5174\u9886\u57df\u9762\u4e34\u7740\u6570\u4ee5\u5343\u8ba1\u7684\u4e25\u5cfb\u6311\u6218\u3001\u96be\u9898\u548c\u56f0\u60d1\uff0c\u5c3d\u7ba1\u4ed6\u4eec\u53ef\u80fd\u6c38\u8fdc\u65e0\u6cd5\u89e3\u51b3\u5176\u4e2d\u7684\u4e00\u4e9b\u95ee\u9898\uff0c\u4f46\u4ed6\u4eec\u6c38\u8fdc\u4e0d\u4f1a\u611f\u5230\u65e0\u804a\u3002\u5bf9\u6570\u5b66\u5bb6\u6765\u8bf4\uff0c\u8fd9\u5df2\u7ecf\u8db3\u591f\u4e86\u3002</p>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/","title":"Learning Path Recommendation Based on Knowledge Tracing Model andReinforcement Learning","text":""},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#abstract","title":"Abstract","text":"<p>\u4ecb\u7ecd\u4ed6\u4eec\u4f7f\u7528\u4e86\u67d0\u4e2aKT\u7b97\u6cd5\u548cRL\u7b97\u6cd5\u6765record the knowledge level change during the learning process.</p>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#introduction","title":"Introduction","text":"<p>\u5927\u6982\u610f\u601d\u5c31\u662f\u8bf4\uff0c\u4f5c\u8005\u8ba4\u4e3a\uff1a\u5927\u591a\u6570\u73b0\u5728\u7684\u5b66\u4e60\u8def\u5f84\u63a8\u8350\uff0c\u90fd\u662f\u7531\u552f\u4e00\u7684\u7ef4\u5ea6\uff08learning costs\uff09\u6765\u8bc4\u5224\uff0c\u800c\u4e14\u8fd9\u4e2a\u552f\u4e00\u7684\u7ef4\u5ea6\u4e5f\u662f\u7531expert\u6765\u624b\u52a8\u6807\u6ce8\u3002</p> <p>\u800c\u4f5c\u8005\u7684\u7b97\u6cd5Knowledge Tracing based Knowledge Demand Model (KTKDM) \u6709\u4e0b\u9762\u51e0\u4e2a\u4eae\u70b9\uff1a</p> <p></p> <p>\u6ca1\u6709\u4ec0\u4e48\u7279\u522b\u5173\u6ce8\u5230\u7684\u70b9</p>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#related-work","title":"Related Work","text":"<p>\u5206\u522b\u4ecb\u7ecd\u4e86BKT\u548cDKT</p>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#proposed-method","title":"Proposed Method","text":""},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#algorithm-overview","title":"Algorithm Overview","text":"<p>\u5148\u8bf4\u4e00\u4e0b\u5efa\u6a21\u90e8\u5206\uff1a</p> <p>\u4f5c\u8005\u63d0\u51fa\u4e86\u4ed6\u4eec\u7684model \u2014\u2014 KT-KDM  </p> <p>KT-KDM\u4e5f\u5206\u4e3a\u4e24\u4e2a\u4e0d\u540c\u7684\u6a21\u5757 \u4e00\u4e2a\u662fKnowledge Demand Model (KDM) \u53e6\u4e00\u4e2a\u662fKnowledge Tracing Model (KTM)</p> <p>KTM \u901a\u8fc7learner\u7684n\u6b21\u7684trajectories\u6765\u5b66\u4e60\u5230KCs \u7136\u540e\u628aKCs\u9001\u7ed9KDM</p> <p>\u7136\u540e\u5c31\u6784\u5efa\u51faMDP</p> <p></p> <p>\u5177\u4f53\u6d41\u7a0b\uff1a</p> <p></p> <p>env\u4e2d KTM\u548cLearner\u4e4b\u95f4\u7684\u9884\u5904\u7406\u611f\u89c9\u5f88\u5389\u5bb3</p>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#knowledge-tracing-model","title":"Knowledge Tracing Model","text":"<p>KTM\u5728\u4f7f\u7528\u524d\u4e5f\u8981\u88ab\u8bad\u7ec3\uff0c\u4e4b\u540e\u624d\u80fd\u4e0a\u5b9e\u6218</p> <ol> <li>\u5148\u5c06KTM\u8bd5\u505a\u4e3aLearner\u4e0eKDM\u4ea4\u4e92 \uff0c\u56e0\u4e3a\u5f00\u59cbKTM\u6709\u8fc7\u53bbLearners\u5f88\u591a\u7684trajectories\u6240\u4ee5\u53ef\u4ee5\u89c6\u4f5c\u4e3a\u4e00\u4e2a\u5f88\u5927\u7684learner's  knowledge database\uff0c\u901a\u8fc7KDM \u8bad\u7ec3\u51fa\u4e00\u4e2a\u65b0\u7684KTM</li> <li>\u5728\u7ebf\u5e73\u53f0\u5c31\u662f\u5b9e\u6218\u4e86\uff0c\u5728\u7ebf\u90e8\u7f72\u9636\u6bb5\uff0c\u771f\u5b9e\u7528\u6237\u4e0e\u6a21\u578b\u8fdb\u884c\u4ea4\u4e92\u3002\u7528\u6237\u83b7\u53d6\u5e76\u5b66\u4e60\u7cfb\u7edf\u4e2d\u63a8\u8350\u7684KC\uff0c\u8fdb\u884c\u5b66\u4e60\u3002\u5b9e\u9645\u5b66\u4e60\u7ed3\u679c\u5408\u5e76\u5230\u5b66\u4e60\u5386\u53f2\u4e2d\uff0c\u4f20\u9012\u7ed9KTM\u8fdb\u884c\u9884\u6d4b</li> </ol> <p></p> <p>\u5173\u4e8eKTM\u662f\u5565 \u5982\u4f55\u5b9e\u73b0\u7684\uff0c\u4f5c\u8005\u7684\u89e3\u91ca\u662f\u4ed6\u4eec\u7684KTM\u5176\u5b9e\u662f\u6b63\u5219\u5316DKT+</p> <p>\u5173\u4e8e\u4ec0\u4e48\u662f\u6b63\u5219\u5316DKT+</p> <ul> <li>\u5f15\u5165Recurrent Neural Networks(RNNs)\u548cLSTM\u7ec6\u80de\u6765\u6a21\u62df\u5b66\u751f\u7684\u77e5\u8bc6\u72b6\u6001\uff0c\u63d0\u51fa\u4e86Deep Knowledge Tracing (DKT)\u6a21\u578b\uff0c\u800cDKT+\u6a21\u578b\u5219\u8fdb\u4e00\u6b65\u8003\u8651\u4e86\u65f6\u95f4\u4e0a\u7684\u4f9d\u8d56\u6027\u3002</li> <li>\u5f15\u5165\u6b63\u5219\u5316\u9879\u5230\u539f\u59cbDKT\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u5305\u62ec\u91cd\u6784\u8bef\u5dee\u548c\u6ce2\u72b6\u5ea6\u91cf\uff0c\u4ee5\u589e\u5f3a\u4e00\u81f4\u9884\u6d4b\u3002</li> <li>\u63d0\u51fa\u4e86\u6027\u80fd\u6307\u6807\u6765\u8bc4\u4f30\u77e5\u8bc6\u8ddf\u8e2a\u7684\u4e09\u4e2a\u65b9\u9762\u7684\u6027\u80fd\u3002</li> </ul>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#learning-path-generation","title":"Learning Path Generation","text":""},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#1-model","title":"1. Model","text":"<ul> <li> <p>\u8be5\u65b9\u6cd5\u5177\u6709\u4e24\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff1a\u77e5\u8bc6\u9700\u6c42\u6a21\u578b\uff08KDM\uff09\u548c\u6743\u91cd\u968f\u673a\u9009\u62e9\u51fd\u6570\u3002</p> </li> <li> <p>KDM\u662f\u4e00\u4e2a\u9884\u6d4b\u7f51\u7edc\uff0c\u5b83\u63a5\u6536\u5b66\u4e60\u8005\u638c\u63e1\u7a0b\u5ea6\u7684\u8f93\u5165\uff0c\u5e76\u8f93\u51fa\u4e00\u4e2a\u8868\u793a\u63a8\u8350KC\u7684\u9884\u6d4b\u6982\u7387\u5411\u91cf\u3002</p> </li> <li>\u6743\u91cd\u968f\u673a\u9009\u62e9\u51fd\u6570\u8fdb\u884c\u63a8\u8350\u9009\u62e9\u3002</li> </ul>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#2-reward","title":"2. Reward","text":"<ul> <li>\u5956\u52b1\u51fd\u6570reward\u8bbe\u8ba1\u4e3a\u76f8\u90bb\u63a8\u8350KC\u7684\u5b66\u4e60\u8005\u540e\u5b66\u4e60\u638c\u63e1\u7a0b\u5ea6\u4e4b\u5dee\uff0c\u5bf9\u4e8e\u5df2\u7ecf\u63a8\u8350\u4e86\u7684KC\u7684\u51cf\u5206\u3002</li> </ul>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#3-training-method","title":"3. Training method","text":"<ul> <li>\u5229\u7528\u4f18\u52bf\u6f14\u5458-\u8bc4\u8bba\u5bb6\uff08A2C\uff09RL\u65b9\u6cd5\u89e3\u51b3\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\u3001\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u548c\u5b9e\u9a8c\u957f\u5ea6\u6709\u9650\u7684\u95ee\u9898\u3002</li> </ul>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#experiments","title":"Experiments","text":""},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#_1","title":"Learning Path Recommendation Based on Knowledge Tracing Model andReinforcement Learning","text":"<ul> <li>\u5728ASSITment 2009-2010\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u4e0e\u8d2a\u5fc3\u548c\u968f\u673a\u7b97\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8868\u660e\u4e86\u6240\u63d0\u51fa\u7684KT-KDM\u65b9\u6cd5\u6bd4\u968f\u673a\u7b97\u6cd5\u66f4\u597d\uff0c\u4e0e\u8d2a\u5fc3\u7b97\u6cd5\u6301\u5e73\uff0c\u540c\u65f6\u8fd0\u7b97\u6210\u672c\u66f4\u4f4e\u3002</li> <li>\u63a8\u8350\u7684KC\u8986\u76d6\u5927\u591a\u6570\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u4e14\u63a8\u8350\u6b21\u6570\u7684\u5206\u5e03\u76f8\u5bf9\u5747\u5300\u3002</li> <li>\u8be5\u6a21\u578b\u5728\u63a8\u8350\u6548\u679c\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u90fd\u5b9e\u7528\u53ef\u884c\u3002</li> </ul>"},{"location":"Learning%20Path%20Recommendation%20Based%20on%20Knowledge%20Tracing%20Model%20andReinforcement%20Learning/#conclusion","title":"Conclusion","text":"<ul> <li>a. \u8be5\u5de5\u4f5c\u7684\u610f\u4e49\uff1a</li> <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u8ddf\u8e2a\u6a21\u578b( KTM)\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u578b(RLM)\u76f8\u7ed3\u5408\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u8def\u5f84\u63a8\u8350\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u52a8\u6001\u8ddf\u8e2a\u5b66\u4e60\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u751f\u6210\u4e86\u8986\u76d6\u5173\u952e\u4e3b\u9898\u7684\u5b66\u4e60\u8def\u5f84\u3002</li> <li>b. \u521b\u65b0\u6027\u3001\u6027\u80fd\u548c\u5de5\u4f5c\u91cf\uff1a</li> <li>\u8be5\u65b9\u6cd5\u8d8a\u8fc7\u4e86\u83b7\u53d6\u5b66\u4e60\u6210\u672c\u7684\u6b65\u9aa4\uff0c\u63d0\u4f9b\u4e86\u66f4\u52a0\u4e2a\u6027\u5316\u7684\u5b66\u4e60\u8def\u5f84\u63a8\u8350\uff0c\u53ef\u4ee5\u5728\u672a\u6709\u5b66\u4e60\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63a8\u8350\u9002\u5f53\u7684\u5b66\u4e60\u8def\u5f84\uff0c\u6700\u7ec8\u5efa\u7acb\u4e86\u5b66\u4e60\u8def\u5f84\u63a8\u8350\u6846\u67b6\u3002</li> <li>c. \u7814\u7a76\u7ed3\u8bba\uff08\u5217\u51fa\u70b9\uff09\uff1a</li> <li>\u5bf9\u4e8e\u63a8\u8350\u7b97\u6cd5\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u77e5\u8bc6\u9700\u6c42\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5b66\u4e60\u8def\u5f84\u63a8\u8350\u7b97\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u751f\u6210\u4e2a\u6027\u5316\u5b66\u4e60\u8def\u5f84\u7684\u65b9\u5411\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u672a\u6765\u5728\u7ebf\u5b66\u4e60\u7684\u8bad\u7ec3\u5e94\u8be5\u540c\u65f6\u8bad\u7ec3\u8fd9\u4e24\u4e2a\u6a21\u578b\u3002</li> </ul>"},{"location":"Playing%20Atari%20with%20Deep%20Reinforcement%20Learning%28DQN%20key%20paper%29/","title":"Playing Atari with Deep Reinforcement Learning(DQN key paper)","text":"<p>note by GuoChengWu  </p>"},{"location":"Playing%20Atari%20with%20Deep%20Reinforcement%20Learning%28DQN%20key%20paper%29/#introduction","title":"Introduction","text":"<p>\u4f5c\u8005\u6307\u51fa\u4ed6\u4eec\u4f7f\u7528\u4e86\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u5e94\u5bf9\u76d1\u7763\u5b66\u4e60\u5728RL\u4e0a\u9762\u7684\u5f31\u52bf\uff0c\u8be5\u7f51\u7edc\u4f7f\u7528 Q-learning \u7b97\u6cd5\u7684\u53d8\u4f53\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SGD)\u6765\u66f4\u65b0\u6743\u91cd\u3002</p> <p>Input\u662f\u4e00\u4e2ahigh dimensional visual input (210 * 160 RGB video at 60Hz)\uff0c\u4e14\u8fd9\u4e2anetwork\u4e0d\u77e5\u9053\u4efb\u4f55\u6709\u5173\u4e8e\u6e38\u620f\u548c\u6a21\u62df\u5668\u5185\u90e8\u7684\u5185\u5bb9</p>"},{"location":"Playing%20Atari%20with%20Deep%20Reinforcement%20Learning%28DQN%20key%20paper%29/#background","title":"Background","text":"<p>\u7531\u4e8e\u8003\u8651\u5230\u53ea\u89c2\u6d4b\u5230\u5f53\u524d\u6e38\u620f\u72b6\u6001(Xt)\u5e76\u4e0d\u8db3\u4ee5\u5206\u6790\u8fd9\u4e00\u72b6\u6001\u7684\u4ef7\u503c\uff0c\u6240\u4ee5\u8003\u8651\u5206\u6790\u4e00\u6574\u4e2asequences\uff08x1,a1,x2...at-1,xt) \u4e14\u5b9a\u4e49\u4e3a s\uff0c\u8fd9\u79cd\u5f62\u5f0f\u4e3b\u4e49\u4ea7\u751f\u4e86\u4e00\u4e2a\u5e9e\u5927\u4f46\u6709\u9650\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b (MDP)\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5e8f\u5217\u90fd\u662f\u4e00\u4e2a\u4e0d\u540c\u7684\u72b6\u6001\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5e94\u7528\u4e8e MDP\uff0c\u53ea\u9700\u4f7f\u7528\u5b8c\u6574\u5e8f\u5217 st \u4f5c\u4e3a\u65f6\u95f4 t \u7684\u72b6\u6001\u8868\u793a\u3002</p> <p>\u6211\u4eec\u7684\u4efb\u52a1\u662f\u8981\u8ba9agent\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u8fc7\u7a0b\u4e2d\u5f97\u5230\u6700\u4f18\u89e3</p> <p></p> <p>\u540c\u6837\u6ee1\u8db3\u8d1d\u5c14\u66fc\u516c\u5f0f\uff08\u5c31\u662f\u4e00\u4e2aQ-learning\uff09</p> <p>loss function\uff1a</p> <p></p> <p>\u8fd9\u4e2a\u7b97\u6cd5\u662fmodel-free\u7684 \u4e5f\u662foff-policy\u7684</p>"},{"location":"Playing%20Atari%20with%20Deep%20Reinforcement%20Learning%28DQN%20key%20paper%29/#related-work","title":"Related work","text":"<p>\u6700\u8457\u540d\u7684\u7684\u4e00\u4e2a\u4efb\u52a1\u662fTD-gammon  \u7528\u5f3a\u5316\u5b66\u4e60\u6765\u73a9\u897f\u6d0b\u53cc\u9646\u68cb</p> <p></p> <p>\u4f5c\u8005\u6307\u51fa\uff1aoff-policy\uff0cmodel-free\u7684function approximator\u8868\u73b0\u7684\u90fd\u5f88\u5dee\uff08\u53d1\u6563\uff09 </p> <p>\u6700\u63a5\u8fd1\u6b64\u4efb\u52a1\u7684\u4e00\u4e2a\u7b97\u6cd5\u2014\u2014NFQ</p> <p></p> <p>\u4f46\u8fd9\u4e2a\u7b97\u6cd5\u7528\u4e86batch gradient descent \u800c DQN\u7528\u7684\u662f stochastic gradient descent</p>"},{"location":"Playing%20Atari%20with%20Deep%20Reinforcement%20Learning%28DQN%20key%20paper%29/#deep-reinforcement-learning","title":"Deep Reinforcement Learning","text":"<p>\u8fd9\u7ae0\u4ecb\u7ecd\u4e86DQN\u7684\u5b9e\u73b0\u6280\u672f</p> <p>\u7531\u4e8e\u8fd1\u5e74\u6765DL\u7684\u53d1\u5c55\u8fc5\u901f\uff0c\u4f5c\u8005\u4eceCV\u548cSC\u7684\u6a21\u578b\u4e2d\u5b66\u4e60\u5230\uff0c\u8ba4\u4e3aSGD\u4f1a\u662f\u5f88\u597d\u7684\u4e00\u4e2a\u7a81\u7834\u53e3</p> <p></p> <p>DQN\u7684\u6838\u5fc3\u6280\u672f\uff1a</p> <ul> <li>experience replay \uff08we store the agent\u2019s experiences at each time-step</li> <li>replay memory</li> </ul> <p></p> <p>\u4f7f\u7528\u8fd9\u4e24\u4e2a\u529e\u6cd5\u4e00\u4e2a\u662f\u5f88\u597d\u7684\u589e\u52a0\u4e86\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u7387\uff0c\u4e8c\u662f\u8fd9\u6837\u8fd9\u4e48\u505a\uff0c\u4e00\u5b9a\u5c31\u662foff-policy\u7684\u4e86\uff0c\u4e5f\u6539\u53d8\u4e86\u5f53\u524d\u53c2\u6570\u4f1a\u51b3\u5b9a\u4e0b\u4e00\u4e2a\u65f6\u523b\u7684data sample\u7684\u53c2\u6570</p> <p></p> <p>\u5b9e\u673a\u6f14\u793a\u4e2d\uff1a</p> <p></p>"},{"location":"Reward%E7%9A%84%E4%B8%80%E4%BA%9B%E9%80%89%E6%8B%A9%28The%20Reward%20Hypothesis%29/","title":"Reward\u7684\u4e00\u4e9b\u9009\u62e9(The Reward Hypothesis)","text":""},{"location":"Reward%E7%9A%84%E4%B8%80%E4%BA%9B%E9%80%89%E6%8B%A9%28The%20Reward%20Hypothesis%29/#reward","title":"Reward\u7684\u4e00\u4e9b\u9009\u62e9...","text":"<p>\u8be6\u7ec6\u770b\u6b64\u89c6\u9891\u5e03\u6717\u5927\u5b66Michael Littman: The Reward Hypothesis</p> <p>\u6211\u4eec\u77e5\u9053Agent\u603b\u662f\u5b66\u4e60\u5982\u4f55\u6700\u5927\u5316\u6536\u76ca\u3002\u5982\u679c\u6211\u4eec\u60f3\u8981\u5b83\u4e3a\u6211\u4eec\u505a\u67d0\u4ef6\u4e8b\u60c5\uff0c\u6211\u4eec\u63d0\u4f9b\u6536\u76ca\u7684\u65b9\u5f0f\u5fc5\u987b\u8981\u4f7f\u5f97Agent\u5728\u6700\u5927\u5316\u6536\u76ca\u7684\u540c\u65f6\u4e5f\u540c\u65f6\u5b9e\u73b0\u6211\u4eec\u7684\u76ee\u6807\u3002</p> <p>You can see what is happening in all of these examples. The agent always learns to maximize its reward. If we want it to do something for us, we must provide rewards to it in such a way that in maximizing them the agent will also achieve our goals. It is thus critical that the rewards we set up truly indicate what we want accomplished. In particular, the reward signal is not the place to impart to the agent prior knowledge about how to achieve what we want it to do.5 For example, a chess-playing agent should be rewarded only for actually winning, not for achieving subgoals such as taking its opponent\u2019s pieces or gaining control of the center of the board. If achieving these sorts of subgoals were rewarded, then the agent might find a way to achieve them without achieving the real goal. For example, it might find a way to take the opponent\u2019s pieces even at the cost of losing the game. The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved.</p>"},{"location":"%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%8A%BD%E8%B1%A1%E6%A6%82%E5%BF%B5%E5%92%8C%E6%8A%80%E6%9C%AF/","title":"\u5f3a\u5316\u5b66\u4e60\u62bd\u8c61\u6982\u5ff5\u548c\u6280\u672f","text":"<p>\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u62bd\u8c61\u6982\u5ff5\u548c\u6280\u672f\u4e3b\u8981\u6307\u5c06\u5e95\u5c42\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u62bd\u8c61\u4e3a\u66f4\u9ad8\u5c42\u6b21\u7684\u8868\u793a\uff0c\u4ee5\u964d\u4f4e\u95ee\u9898\u7684\u590d\u6742\u6027\u3002\u4ee5\u4e0b\u662f\u4e00\u4e9b\u5173\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u62bd\u8c61\u6982\u5ff5\u548c\u6280\u672f\uff1a</p> <p>\u4ee5\u4e0b\u6765\u81eaGPT-4</p> <ol> <li>\u62bd\u8c61\u72b6\u6001\uff08Abstract State\uff09\uff1a\u5c06\u539f\u59cb\u72b6\u6001\u7a7a\u95f4\u6620\u5c04\u5230\u4e00\u4e2a\u66f4\u9ad8\u5c42\u6b21\u7684\u72b6\u6001\u7a7a\u95f4\uff0c\u4ece\u800c\u964d\u4f4e\u72b6\u6001\u7a7a\u95f4\u7684\u5927\u5c0f\u548c\u590d\u6742\u6027\u3002\u4f8b\u5982\uff0c\u5728\u68cb\u76d8\u6e38\u620f\u4e2d\uff0c\u53ef\u4ee5\u5c06\u68cb\u5b50\u7684\u5177\u4f53\u4f4d\u7f6e\u62bd\u8c61\u4e3a\u68cb\u5b50\u7684\u7c7b\u578b\u548c\u6570\u91cf\u3002</li> <li>\u62bd\u8c61\u52a8\u4f5c\uff08Abstract Action\uff09\uff1a\u5c06\u539f\u59cb\u52a8\u4f5c\u7a7a\u95f4\u6620\u5c04\u5230\u4e00\u4e2a\u66f4\u9ad8\u5c42\u6b21\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4ee5\u964d\u4f4e\u52a8\u4f5c\u7a7a\u95f4\u7684\u5927\u5c0f\u548c\u590d\u6742\u6027\u3002\u4f8b\u5982\uff0c\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u53ef\u4ee5\u5c06\u8fde\u7eed\u7684\u5173\u8282\u89d2\u5ea6\u62bd\u8c61\u4e3a\u79bb\u6563\u7684\u79fb\u52a8\u65b9\u5411\u3002</li> <li>\u9009\u9879\u6846\u67b6\uff08Option Framework\uff09\uff1a\u9009\u9879\uff08Options\uff09\u662f\u4e00\u79cd\u7528\u4e8e\u8868\u793a\u591a\u6b65\u884c\u4e3a\u7684\u62bd\u8c61\u6982\u5ff5\uff0c\u5b83\u4eec\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4ecb\u4e8e\u539f\u59cb\u52a8\u4f5c\u548c\u9ad8\u5c42\u7b56\u7565\u4e4b\u95f4\u7684\u4e00\u79cd\u7ed3\u6784\u3002\u9009\u9879\u6846\u67b6\u53ef\u4ee5\u5e2e\u52a9\u667a\u80fd\u4f53\u5728\u66f4\u9ad8\u5c42\u6b21\u4e0a\u8fdb\u884c\u51b3\u7b56\uff0c\u4ece\u800c\u964d\u4f4e\u95ee\u9898\u7684\u590d\u6742\u6027\u3002</li> <li>\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\uff08Hierarchical Reinforcement Learning\uff0c\u7b80\u79f0 HRL\uff09\uff1a\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u5b50\u4efb\u52a1\u53ef\u4ee5\u4f7f\u7528\u66f4\u7b80\u5355\u7684\u7b56\u7565\u6765\u89e3\u51b3\u3002\u901a\u8fc7\u8fd9\u79cd\u5206\u5c42\u7ed3\u6784\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u5728\u66f4\u62bd\u8c61\u7684\u5c42\u6b21\u4e0a\u8fdb\u884c\u5b66\u4e60\u548c\u51b3\u7b56\uff0c\u4ece\u800c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002</li> <li>\u72b6\u6001\u805a\u7c7b\uff08State Clustering\uff09\uff1a\u72b6\u6001\u805a\u7c7b\u662f\u4e00\u79cd\u5c06\u76f8\u4f3c\u72b6\u6001\u805a\u5408\u5728\u4e00\u8d77\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u964d\u4f4e\u72b6\u6001\u7a7a\u95f4\u7684\u5927\u5c0f\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u805a\u7c7b\u7b97\u6cd5\u6216\u8005\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u5b9e\u73b0\uff0c\u5982\u81ea\u7f16\u7801\u5668\uff08Autoencoders\uff09\u6216\u8005\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08Variational Autoencoders\uff09\u3002</li> </ol> <p>\u4e2a\u4eba\u770b\u6cd5\uff1a</p> <p>\u66f4\u50cf\u4eba\u7c7b\u601d\u8003\u95ee\u9898\u7684\u65b9\u5f0f</p>"},{"location":"%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%8A%BD%E8%B1%A1%E6%A6%82%E5%BF%B5%E5%92%8C%E6%8A%80%E6%9C%AF/#eg","title":"e.g","text":"<p>\u6bd4\u5982\u4e3e\u4e2a\u4f8b\u5b50\uff1a</p> <p>\u5728\u8fd9\u4e2a\u4efb\u52a1\u4e2d\uff0c\u667a\u80fd\u4f53\uff08\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff09\u7684\u76ee\u6807\u662f\u5728\u9ad8\u901f\u516c\u8def\u4e0a\u4ee5\u5b89\u5168\u3001\u9ad8\u6548\u7684\u65b9\u5f0f\u884c\u9a76\u3002\u539f\u59cb\u72b6\u6001\u7a7a\u95f4\u53ef\u80fd\u5305\u62ec\u6c7d\u8f66\u7684\u4f4d\u7f6e\u3001\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u3001\u5468\u56f4\u8f66\u8f86\u7684\u4fe1\u606f\u7b49\u3002\u539f\u59cb\u52a8\u4f5c\u7a7a\u95f4\u53ef\u80fd\u5305\u62ec\u6cb9\u95e8\u3001\u5239\u8f66\u548c\u65b9\u5411\u76d8\u89d2\u5ea6\u7b49\u8fde\u7eed\u63a7\u5236\u4fe1\u53f7\u3002</p> <ol> <li>\u62bd\u8c61\u72b6\u6001\uff1a\u6211\u4eec\u53ef\u4ee5\u5c06\u6c7d\u8f66\u7684\u4f4d\u7f6e\u62bd\u8c61\u4e3a\u8f66\u9053\u7f16\u53f7\uff0c\u5c06\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u62bd\u8c61\u4e3a\u79bb\u6563\u7684\u901f\u5ea6\u7ea7\u522b\uff0c\u5c06\u5468\u56f4\u8f66\u8f86\u7684\u4fe1\u606f\u62bd\u8c61\u4e3a\u76f8\u5bf9\u8ddd\u79bb\u548c\u76f8\u5bf9\u901f\u5ea6\u3002\u8fd9\u6837\uff0c\u72b6\u6001\u7a7a\u95f4\u5c06\u53d8\u5f97\u66f4\u5c0f\u3001\u66f4\u6613\u4e8e\u5904\u7406\u3002</li> <li>\u62bd\u8c61\u52a8\u4f5c\uff1a\u6211\u4eec\u53ef\u4ee5\u5c06\u8fde\u7eed\u7684\u6cb9\u95e8\u3001\u5239\u8f66\u548c\u65b9\u5411\u76d8\u89d2\u5ea6\u62bd\u8c61\u4e3a\u79bb\u6563\u7684\u52a8\u4f5c\uff0c\u5982\u201c\u52a0\u901f\u201d\u3001\u201c\u51cf\u901f\u201d\u3001\u201c\u4fdd\u6301\u901f\u5ea6\u201d\u3001\u201c\u53d8\u9053\u5de6\u201d\u548c\u201c\u53d8\u9053\u53f3\u201d\u3002\u8fd9\u6837\u53ef\u4ee5\u964d\u4f4e\u52a8\u4f5c\u7a7a\u95f4\u7684\u590d\u6742\u6027\uff0c\u7b80\u5316\u7b56\u7565\u5b66\u4e60\u3002</li> <li>\u9009\u9879\u6846\u67b6\uff1a\u6211\u4eec\u53ef\u4ee5\u4e3a\u667a\u80fd\u4f53\u8bbe\u8ba1\u4e00\u4e9b\u9ad8\u5c42\u6b21\u7684\u9009\u9879\uff0c\u5982\u201c\u8ddf\u8f66\u201d\u3001\u201c\u8d85\u8f66\u201d\u548c\u201c\u907f\u8ba9\u201d\u3002\u6bcf\u4e2a\u9009\u9879\u53ef\u4ee5\u5305\u542b\u4e00\u7cfb\u5217\u4f4e\u5c42\u6b21\u7684\u52a8\u4f5c\u5e8f\u5217\u3002\u667a\u80fd\u4f53\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u5728\u4e0d\u540c\u60c5\u51b5\u4e0b\u9009\u62e9\u54ea\u4e2a\u9009\u9879\u6765\u63d0\u9ad8\u5176\u51b3\u7b56\u80fd\u529b\u3002</li> <li>\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\uff1a\u6211\u4eec\u53ef\u4ee5\u5c06\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a\u8f66\u9053\u4fdd\u6301\u548c\u8f66\u8f86\u4ea4\u4e92\u3002\u8f66\u9053\u4fdd\u6301\u5b50\u4efb\u52a1\u4e3b\u8981\u5173\u6ce8\u7ef4\u6301\u6b63\u786e\u7684\u8f66\u9053\u548c\u901f\u5ea6\uff0c\u800c\u8f66\u8f86\u4ea4\u4e92\u5b50\u4efb\u52a1\u5219\u5173\u6ce8\u4e0e\u5176\u4ed6\u8f66\u8f86\u7684\u4ea4\u4e92\uff0c\u5982\u8d85\u8f66\u3001\u8ddf\u8f66\u548c\u907f\u8ba9\u3002\u901a\u8fc7\u8fd9\u79cd\u5206\u5c42\u7ed3\u6784\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u5728\u66f4\u62bd\u8c61\u7684\u5c42\u6b21\u4e0a\u8fdb\u884c\u5b66\u4e60\u548c\u51b3\u7b56\u3002</li> <li>\u72b6\u6001\u805a\u7c7b\uff1a\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u805a\u7c7b\u7b97\u6cd5\u5c06\u76f8\u4f3c\u7684\u72b6\u6001\u805a\u5408\u5728\u4e00\u8d77\uff0c\u5982\u5c06\u5177\u6709\u76f8\u4f3c\u8ddd\u79bb\u548c\u76f8\u5bf9\u901f\u5ea6\u7684\u90bb\u8fd1\u8f66\u8f86\u8fdb\u884c\u805a\u7c7b\u3002\u8fd9\u6837\u53ef\u4ee5\u964d\u4f4e\u72b6\u6001\u7a7a\u95f4\u7684\u5927\u5c0f\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002</li> <li>\u8f6c\u79fb\u5b66\u4e60\uff1a\u5982\u679c\u6211\u4eec\u5df2\u7ecf\u8bad\u7ec3\u4e86\u4e00\u4e2a\u667a\u80fd\u4f53\u5728\u57ce\u5e02\u9053\u8def\u4e0a\u884c\u9a76\uff0c\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u5728\u8fd9\u4e2a\u4efb\u52a1\u4e0a\u5b66\u5230\u7684\u77e5\u8bc6\u6765\u52a0\u901f\u9ad8\u901f\u516c\u8def\u4e0a\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5df2\u5b66\u5230\u7684\u8f66\u9053\u4fdd\u6301\u7b56\u7565\u5e94\u7528\u4e8e\u65b0\u4efb\u52a1\uff0c\u53ea\u9700\u5bf9\u8f66\u8f86\u4ea4\u4e92\u7b56\u7565\u8fdb\u884c\u5fae\u8c03\u5373\u53ef\u3002</li> </ol> <p>\u901a\u8fc7\u8fd9\u4e9b\u6280\u672f\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u590d\u6742\u7684\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u3001\u66f4\u6613\u4e8e\u5904\u7406\u7684\u5b50\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u667a\u80fd\u4f53\u66f4\u5feb\u5730\u5b66\u4e60\u5230\u4f18\u79c0\u7684\u7b56\u7565\uff0c\u5e76\u66f4\u597d\u5730\u5e94\u5bf9\u4e0d\u540c\u7684\u9a7e\u9a76\u73af\u5883\u548c\u6761\u4ef6\u3002</p> <p>\u6b64\u5916\uff0c\u8fd9\u4e9b\u62bd\u8c61\u6982\u5ff5\u548c\u6280\u672f\u8fd8\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u4f7f\u7528\u9009\u9879\u6846\u67b6\u548c\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u89c2\u5730\u4e86\u89e3\u667a\u80fd\u4f53\u662f\u5982\u4f55\u6839\u636e\u4e0d\u540c\u7684\u60c5\u51b5\u9009\u62e9\u4e0d\u540c\u7684\u9ad8\u5c42\u7b56\u7565\u7684\u3002\u8fd9\u5c06\u6709\u52a9\u4e8e\u6211\u4eec\u7406\u89e3\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4ece\u800c\u8fdb\u884c\u8c03\u8bd5\u548c\u4f18\u5316\u3002</p> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u6839\u636e\u5177\u4f53\u95ee\u9898\u548c\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u62bd\u8c61\u65b9\u6cd5\u3002\u8fc7\u5ea6\u62bd\u8c61\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u4ece\u800c\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff1b\u800c\u8fc7\u5ea6\u7ec6\u5316\u5219\u53ef\u80fd\u5bfc\u81f4\u72b6\u6001\u7a7a\u95f4\u548c\u52a8\u4f5c\u7a7a\u95f4\u8fc7\u5927\uff0c\u589e\u52a0\u5b66\u4e60\u7684\u96be\u5ea6\u3002\u56e0\u6b64\uff0c\u5728\u5b9e\u8df5\u4e2d\u9700\u8981\u6743\u8861\u62bd\u8c61\u5c42\u6b21\u4e0e\u4fdd\u7559\u8db3\u591f\u4fe1\u606f\u4e4b\u95f4\u7684\u5173\u7cfb\u3002</p> <p>\u603b\u4e4b\uff0c\u901a\u8fc7\u5c06\u5e95\u5c42\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u62bd\u8c61\u4e3a\u66f4\u9ad8\u5c42\u6b21\u7684\u8868\u793a\uff0c\u6211\u4eec\u53ef\u4ee5\u964d\u4f4e\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002\u62bd\u8c61\u6982\u5ff5\u548c\u6280\u672f\u5728\u8bb8\u591a\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e2d\u90fd\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u63a7\u5236\u3001\u6e38\u620fAI\u7b49\u9886\u57df\u3002</p>"},{"location":"%20Bandits%20and%20Exploration%3AExploitation%20Assignment/Assignment1/","title":"Assignment 1: Bandits and Exploration/Exploitation","text":"<p>Welcome to Assignment 1. This notebook will: - Help you create your first bandit algorithm - Help you understand the effect of epsilon on exploration and learn about the exploration/exploitation tradeoff - Introduce you to some of the reinforcement learning software we are going to use for this specialization</p> <p>This class uses RL-Glue to implement most of our experiments. It was originally designed by Adam White, Brian Tanner, and Rich Sutton. This library will give you a solid framework to understand how reinforcement learning experiments work and how to run your own. If it feels a little confusing at first, don't worry - we are going to walk you through it slowly and introduce you to more and more parts as you progress through the specialization.</p> <p>We are assuming that you have used a Jupyter notebook before. But if not, it is quite simple. Simply press the run button, or shift+enter to run each of the cells. The places in the code that you need to fill in will be clearly marked for you.</p>"},{"location":"%20Bandits%20and%20Exploration%3AExploitation%20Assignment/Assignment1/#section-0-preliminaries","title":"Section 0: Preliminaries","text":"<pre><code># Import necessary libraries\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\n\nfrom rlglue.rl_glue import RLGlue\nimport main_agent\nimport ten_arm_env\nimport test_env\n</code></pre> <p>In the above cell, we import the libraries we need for this assignment. We use numpy throughout the course and occasionally provide hints for which methods to use in numpy. Other than that we mostly use vanilla python and the occasional other library, such as matplotlib for making plots.</p> <p>You might have noticed that we import ten_arm_env. This is the 10-armed Testbed introduced in section 2.3 of the textbook. We use this throughout this notebook to test our bandit agents. It has 10 arms, which are the actions the agent can take. Pulling an arm generates a stochastic reward from a Gaussian distribution with unit-variance. For each action, the expected value of that action is randomly sampled from a normal distribution, at the start of each run. If you are unfamiliar with the 10-armed Testbed please review it in the textbook before continuing.</p> <p>DO NOT IMPORT OTHER LIBRARIES as this will break the autograder.</p> <p>DO NOT SET A RANDOM SEED as this will break the autograder.</p> <p>Please do not duplicate cells. This will put your notebook into a bad state and break Cousera's autograder.</p> <p>Before you submit, please click \"Kernel\" -&gt; \"Restart and Run All\" and make sure all cells pass.</p>"},{"location":"%20Bandits%20and%20Exploration%3AExploitation%20Assignment/Assignment1/#section-1-greedy-agent","title":"Section 1: Greedy Agent","text":"<p>We want to create an agent that will find the action with the highest expected reward. One way an agent could operate is to always choose the action with  the highest value based on the agent\u2019s current estimates. This is called a greedy agent as it greedily chooses the action that it thinks has the highest value. Let's look at what happens in this case.</p> <p>First we are going to implement the argmax function, which takes in a list of action values and returns an action with the highest value. Why are we implementing our own instead of using the argmax function that numpy uses? Numpy's argmax function returns the first instance of the highest value. We do not want that to happen as it biases the agent to choose a specific action in the case of ties. Instead we want to break ties between the highest values randomly. So we are going to implement our own argmax function. You may want to look at np.random.choice to randomly select from a list of values.</p> <pre><code># -----------\n# Graded Cell\n# -----------\ndef argmax(q_values):\n    \"\"\"\n    Takes in a list of q_values and returns the index of the item \n    with the highest value. Breaks ties randomly.\n    returns: int - the index of the highest value in q_values\n    \"\"\"\n    top_value = float(\"-inf\")\n    ties = []\n\n    for i in range(len(q_values)):\n        # if a value in q_values is greater than the highest value update top and reset ties to zero\n        # if a value is equal to top value add the index to ties\n        # return a random selection from ties.\n        # your code here\n        if q_values[i] &gt; top_value:\n            top_value = q_values[i]\n            ties = [i]\n        elif q_values[i] == top_value:\n            ties.append(i)\n\n    return np.random.choice(ties)\n</code></pre> <pre><code># --------------\n# Debugging Cell\n# --------------\n# Feel free to make any changes to this cell to debug your code\n\ntest_array = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\nassert argmax(test_array) == 8, \"Check your argmax implementation returns the index of the largest value\"\n\n# make sure np.random.choice is called correctly\nnp.random.seed(0)\ntest_array = [1, 0, 0, 1]\n\nassert argmax(test_array) == 0\n</code></pre> <pre><code># -----------\n# Tested Cell\n# -----------\n# The contents of the cell will be tested by the autograder.\n# If they do not pass here, they will not pass there.\n\ntest_array = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\nassert argmax(test_array) == 8, \"Check your argmax implementation returns the index of the largest value\"\n\n# set random seed so results are deterministic\nnp.random.seed(0)\ntest_array = [1, 0, 0, 1]\n\ncounts = [0, 0, 0, 0]\nfor _ in range(100):\n    a = argmax(test_array)\n    counts[a] += 1\n\n# make sure argmax does not always choose first entry\nassert counts[0] != 100, \"Make sure your argmax implementation randomly choooses among the largest values.\"\n\n# make sure argmax does not always choose last entry\nassert counts[3] != 100, \"Make sure your argmax implementation randomly choooses among the largest values.\"\n\n# make sure the random number generator is called exactly once whenver `argmax` is called\nexpected = [44, 0, 0, 56] # &lt;-- notice not perfectly uniform due to randomness\nassert counts == expected\n</code></pre> <p>Now we introduce the first part of an RL-Glue agent that you will implement. Here we are going to create a GreedyAgent and implement the agent_step method. This method gets called each time the agent takes a step. The method has to return the action selected by the agent. This method also ensures the agent\u2019s estimates are updated based on the signals it gets from the environment.</p> <p>Fill in the code below to implement a greedy agent.</p> <pre><code># -----------\n# Graded Cell\n# -----------\nclass GreedyAgent(main_agent.Agent):\n    def agent_step(self, reward, observation=None):\n        \"\"\"\n        Takes one step for the agent. It takes in a reward and observation and \n        returns the action the agent chooses at that time step.\n\n        Arguments:\n        reward -- float, the reward the agent recieved from the environment after taking the last action.\n        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n                              until future lessons\n        Returns:\n        current_action -- int, the action chosen by the agent at the current time step.\n        \"\"\"\n        ### Useful Class Variables ###\n        # self.q_values : An array with what the agent believes each of the values of the arm are.\n        # self.arm_count : An array with a count of the number of times each arm has been pulled.\n        # self.last_action : The action that the agent took on the previous time step\n        #######################\n\n        # Update Q values Hint: Look at the algorithm in section 2.4 of the textbook.\n        # increment the counter in self.arm_count for the action from the previous time step\n        # update the step size using self.arm_count\n        # update self.q_values for the action from the previous time step\n\n        # your code here\n        if self.last_action is not None:\n            self.arm_count[self.last_action] += 1\n            step_size = 1.0 / self.arm_count[self.last_action] # alpha\n\n            self.q_values[self.last_action] += step_size * (reward - self.q_values[self.last_action])\n\n        # current action = ? # Use the argmax function you created above\n        # your code here\n        current_action = argmax(self.q_values)\n\n        self.last_action = current_action\n\n        return current_action\n\n</code></pre> <pre><code># --------------\n# Debugging Cell\n# --------------\n# Feel free to make any changes to this cell to debug your code\n\n# build a fake agent for testing and set some initial conditions\nnp.random.seed(1)\ngreedy_agent = GreedyAgent()\ngreedy_agent.q_values = [0, 0, 0.5, 0, 0]\ngreedy_agent.arm_count = [0, 1, 0, 0, 0]\ngreedy_agent.last_action = 1\n\naction = greedy_agent.agent_step(reward=1)\n\n# make sure the q_values were updated correctly\nassert greedy_agent.q_values == [0, 0.5, 0.5, 0, 0]\n\n# make sure the agent is using the argmax that breaks ties randomly\nassert action == 2\n</code></pre> <pre><code># -----------\n# Tested Cell\n# -----------\n# The contents of the cell will be tested by the autograder.\n# If they do not pass here, they will not pass there.\n\n# build a fake agent for testing and set some initial conditions\nnp.random.seed(1)\ngreedy_agent = GreedyAgent()\ngreedy_agent.q_values = [0, 0, 1.0, 0, 0]\ngreedy_agent.arm_count = [0, 1, 0, 0, 0]\ngreedy_agent.last_action = 1\n\n# take a fake agent step\naction = greedy_agent.agent_step(reward=1)\n\n# make sure agent took greedy action\nassert action == 2\n\n# make sure q_values were updated correctly\nassert greedy_agent.q_values == [0, 0.5, 1.0, 0, 0]\n\n# take another step\naction = greedy_agent.agent_step(reward=2)\nassert action == 2\nassert greedy_agent.q_values == [0, 0.5, 2.0, 0, 0]\n</code></pre> <p>Let's visualize the result. Here we run an experiment using RL-Glue to test our agent. For now, we will set up the experiment code; in future lessons, we will walk you through running experiments so that you can create your own.</p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\n\nnum_runs = 200                    # The number of times we run the experiment\nnum_steps = 1000                  # The number of pulls of each arm the agent takes\nenv = ten_arm_env.Environment     # We set what environment we want to use to test\nagent = GreedyAgent               # We choose what agent we want to use\nagent_info = {\"num_actions\": 10}  # We pass the agent the information it needs. Here how many arms there are.\nenv_info = {}                     # We pass the environment the information it needs. In this case nothing.\n\nrewards = np.zeros((num_runs, num_steps))\naverage_best = 0\nfor run in tqdm(range(num_runs)):           # tqdm is what creates the progress bar below\n    np.random.seed(run)\n\n    rl_glue = RLGlue(env, agent)          # Creates a new RLGlue experiment with the env and agent we chose above\n    rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n    rl_glue.rl_start()                    # We start the experiment\n\n    average_best += np.max(rl_glue.environment.arms)\n\n    for i in range(num_steps):\n        reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return\n                                                 # the reward, and action taken.\n        rewards[run, i] = reward\n\ngreedy_scores = np.mean(rewards, axis=0)\nplt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\nplt.plot([average_best / num_runs for _ in range(num_steps)], linestyle=\"--\")\nplt.plot(greedy_scores)\nplt.legend([\"Best Possible\", \"Greedy\"])\nplt.title(\"Average Reward of Greedy Agent\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Average reward\")\nplt.show()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:02&lt;00:00, 71.44it/s]\n</code></pre> <p></p> <p>How did our agent do? Is it possible for it to do better?</p>"},{"location":"%20Bandits%20and%20Exploration%3AExploitation%20Assignment/Assignment1/#section-2-epsilon-greedy-agent","title":"Section 2: Epsilon-Greedy Agent","text":"<p>We learned about another way for an agent to operate, where it does not always take the greedy action. Instead, sometimes it takes an exploratory action. It does this so that it can find out what the best action really is. If we always choose what we think is the current best action is, we may miss out on taking the true best action, because we haven't explored enough times to find that best action.</p> <p>Implement an epsilon-greedy agent below. Hint: we are implementing the algorithm from section 2.4 of the textbook. You may want to use your greedy code from above and look at np.random.random, as well as np.random.randint, to help you select random actions. </p> <pre><code># -----------\n# Graded Cell\n# -----------\nclass EpsilonGreedyAgent(main_agent.Agent):\n    def agent_step(self, reward, observation):\n        \"\"\"\n        Takes one step for the agent. It takes in a reward and observation and \n        returns the action the agent chooses at that time step.\n\n        Arguments:\n        reward -- float, the reward the agent recieved from the environment after taking the last action.\n        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n                              until future lessons\n        Returns:\n        current_action -- int, the action chosen by the agent at the current time step.\n        \"\"\"\n\n        ### Useful Class Variables ###\n        # self.q_values : An array with what the agent believes each of the values of the arm are.\n        # self.arm_count : An array with a count of the number of times each arm has been pulled.\n        # self.last_action : The action that the agent took on the previous time step\n        # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)\n        #######################\n\n        # Update Q values - this should be the same update as your greedy agent above\n        # your code here\n        if self.last_action is not None:\n            self.arm_count[self.last_action] += 1\n            step_size = 1.0 / self.arm_count[self.last_action] # alpha\n\n            self.q_values[self.last_action] += step_size * (reward - self.q_values[self.last_action])\n\n        # Choose action using epsilon greedy\n        # Randomly choose a number between 0 and 1 and see if it's less than self.epsilon\n        # (hint: look at np.random.random()). If it is, set current_action to a random action.\n        # otherwise choose current_action greedily as you did above.\n        # your code here\n        p = np.random.random()\n        if p &lt; self.epsilon:\n            # random action\n            current_action = np.random.choice(self.num_actions)\n        else:\n            current_action = argmax(self.q_values)\n        self.last_action = current_action\n\n        return current_action\n</code></pre> <pre><code># --------------\n# Debugging Cell\n# --------------\n# Feel free to make any changes to this cell to debug your code\n\n# build a fake agent for testing and set some initial conditions\nnp.random.seed(0)\ne_greedy_agent = EpsilonGreedyAgent()\ne_greedy_agent.q_values = [0, 0.0, 0.5, 0, 0]\ne_greedy_agent.arm_count = [0, 1, 0, 0, 0]\ne_greedy_agent.num_actions = 5\ne_greedy_agent.last_action = 1\ne_greedy_agent.epsilon = 0.5\n\n# given this random seed, we should see a greedy action (action 2) here\naction = e_greedy_agent.agent_step(reward=1, observation=0)\n\n# -----------------------------------------------\n# we'll try to guess a few of the trickier places\n# -----------------------------------------------\nprint(e_greedy_agent.q_values)\n# make sure to update for the *last_action* not the current action\nassert e_greedy_agent.q_values != [0, 0.5, 1.0, 0, 0], \"A\"\n\n# make sure the stepsize is based on the *last_action* not the current action\nassert e_greedy_agent.q_values != [0, 1, 0.5, 0, 0], \"B\"\n\n# make sure the agent is using the argmax that breaks ties randomly\nassert action == 2, \"C\"\n\n# -----------------------------------------------\n\n# let's see what happens for another action\nnp.random.seed(1)\ne_greedy_agent = EpsilonGreedyAgent()\ne_greedy_agent.q_values = [0, 0.5, 0.5, 0, 0]\ne_greedy_agent.arm_count = [0, 1, 0, 0, 0]\ne_greedy_agent.num_actions = 5\ne_greedy_agent.last_action = 1\ne_greedy_agent.epsilon = 0.5\n\n# given this random seed, we should see a random action (action 4) here\naction = e_greedy_agent.agent_step(reward=1, observation=0)\n\n# The agent saw a reward of 1, so should increase the value for *last_action*\nassert e_greedy_agent.q_values == [0, 0.75, 0.5, 0, 0], \"D\"\n\n# the agent should have picked a random action for this particular random seed\nassert action == 4, \"E\"\n\n</code></pre> <pre><code>[0, 0.5, 0.5, 0, 0]\n</code></pre> <pre><code># -----------\n# Tested Cell\n# -----------\n# The contents of the cell will be tested by the autograder.\n# If they do not pass here, they will not pass there.\n\nnp.random.seed(0)\ne_greedy_agent = EpsilonGreedyAgent()\ne_greedy_agent.q_values = [0, 0, 1.0, 0, 0]\ne_greedy_agent.arm_count = [0, 1, 0, 0, 0]\ne_greedy_agent.num_actions = 5\ne_greedy_agent.last_action = 1\ne_greedy_agent.epsilon = 0.5\naction = e_greedy_agent.agent_step(reward=1, observation=0)\n\nassert e_greedy_agent.q_values == [0, 0.5, 1.0, 0, 0]\n\n# manipulate the random seed so the agent takes a random action\nnp.random.seed(1)\naction = e_greedy_agent.agent_step(reward=0, observation=0)\n\nassert action == 4\n\n# check to make sure we update value for action 4\naction = e_greedy_agent.agent_step(reward=1, observation=0)\nassert e_greedy_agent.q_values == [0, 0.5, 0.0, 0, 1.0]\n</code></pre> <p>Now that we have our epsilon greedy agent created. Let's compare it against the greedy agent with epsilon of 0.1.</p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\n\n# Plot Epsilon greedy results and greedy results\nnum_runs = 200\nnum_steps = 1000\nepsilon = 0.1\nagent = EpsilonGreedyAgent\nenv = ten_arm_env.Environment\nagent_info = {\"num_actions\": 10, \"epsilon\": epsilon}\nenv_info = {}\nall_rewards = np.zeros((num_runs, num_steps))\n\nfor run in tqdm(range(num_runs)):\n    np.random.seed(run)\n\n    rl_glue = RLGlue(env, agent)\n    rl_glue.rl_init(agent_info, env_info)\n    rl_glue.rl_start()\n\n    for i in range(num_steps):\n        reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return\n                                                 # the reward, and action taken.\n        all_rewards[run, i] = reward\n\n# take the mean over runs\nscores = np.mean(all_rewards, axis=0)\nplt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\nplt.plot([1.55 for _ in range(num_steps)], linestyle=\"--\")\nplt.plot(greedy_scores)\nplt.title(\"Average Reward of Greedy Agent vs. E-Greedy Agent\")\nplt.plot(scores)\nplt.legend((\"Best Possible\", \"Greedy\", \"Epsilon: 0.1\"))\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Average reward\")\nplt.show()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:03&lt;00:00, 64.54it/s]\n</code></pre> <p></p> <p>Notice how much better the epsilon-greedy agent did. Because we occasionally choose a random action we were able to find a better long term policy. By acting greedily before our value estimates are accurate, we risk settling on a suboptimal action.</p>"},{"location":"%20Bandits%20and%20Exploration%3AExploitation%20Assignment/Assignment1/#section-21-averaging-multiple-runs","title":"Section 2.1 Averaging Multiple Runs","text":"<p>Did you notice that we averaged over 200 runs? Why did we do that?</p> <p>To get some insight, let's look at the results of two individual runs by the same agent.</p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\n\n# Plot runs of e-greedy agent\nagent = EpsilonGreedyAgent\nenv = ten_arm_env.Environment\nagent_info = {\"num_actions\": 10, \"epsilon\": 0.1}\nenv_info = {}\nall_averages = []\nplt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\nnum_steps = 1000\n\nfor run in (0, 1):\n    np.random.seed(run) # Here we set the seed so that we can compare two different runs\n    averages = []\n    rl_glue = RLGlue(env, agent)\n    rl_glue.rl_init(agent_info, env_info)\n    rl_glue.rl_start()\n\n    scores = [0]\n    for i in range(num_steps):\n        reward, state, action, is_terminal = rl_glue.rl_step()\n        scores.append(scores[-1] + reward)\n        averages.append(scores[-1] / (i + 1))\n\n    plt.plot(averages)\n\nplt.title(\"Comparing two independent runs\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Average reward\")\nplt.show()\n</code></pre> <p></p> <p>Notice how the two runs were different? But, if this is the exact same algorithm, why does it behave differently in these two runs?</p> <p>The answer is that it is due to randomness in the environment and in the agent. Depending on what action the agent randomly starts with, or when it randomly chooses to explore, it can change the results of the runs. And even if the agent chooses the same action, the reward from the environment is randomly sampled from a Gaussian. The agent could get lucky, and see larger rewards for the best action early on and so settle on the best action faster. Or, it could get unlucky and see smaller rewards for best action early on and so take longer to recognize that it is in fact the best action.</p> <p>To be more concrete, let\u2019s look at how many times an exploratory action is taken, for different seeds. </p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\nprint(\"Random Seed 1\")\nnp.random.seed(1)\nfor _ in range(15):\n    if np.random.random() &lt; 0.1:\n        print(\"Exploratory Action\")\n\n\nprint()\nprint()\n\nprint(\"Random Seed 2\")\nnp.random.seed(2)\nfor _ in range(15):\n    if np.random.random() &lt; 0.1:\n        print(\"Exploratory Action\")\n</code></pre> <pre><code>Random Seed 1\nExploratory Action\nExploratory Action\nExploratory Action\n\n\nRandom Seed 2\nExploratory Action\n</code></pre> <p>With the first seed, we take an exploratory action three times out of 15, but with the second, we only take an exploratory action once. This can significantly affect the performance of our agent because the amount of exploration has changed significantly.</p> <p>To compare algorithms, we therefore report performance averaged across many runs. We do this to ensure that we are not simply reporting a result that is due to stochasticity, as explained in the lectures. Rather, we want statistically significant outcomes. We will not use statistical significance tests in this course. Instead, because we have access to simulators for our experiments, we use the simpler strategy of running for a large number of runs and ensuring that the confidence intervals do not overlap. </p>"},{"location":"%20Bandits%20and%20Exploration%3AExploitation%20Assignment/Assignment1/#section-3-comparing-values-of-epsilon","title":"Section 3: Comparing values of epsilon","text":"<p>Can we do better than an epsilon of 0.1? Let's try several different values for epsilon and see how they perform. We try different settings of key performance parameters to understand how the agent might perform under different conditions.</p> <p>Below we run an experiment where we sweep over different values for epsilon:</p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\n\n# Experiment code for different e-greedy\nepsilons = [0.0, 0.01, 0.1, 0.4]\n\nplt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\nplt.plot([1.55 for _ in range(num_steps)], linestyle=\"--\")\n\nn_q_values = []\nn_averages = []\nn_best_actions = []\n\nnum_runs = 200\n\nfor epsilon in epsilons:\n    all_averages = []\n    for run in tqdm(range(num_runs)):\n        agent = EpsilonGreedyAgent\n        agent_info = {\"num_actions\": 10, \"epsilon\": epsilon}\n        env_info = {\"random_seed\": run}\n\n        rl_glue = RLGlue(env, agent)\n        rl_glue.rl_init(agent_info, env_info)\n        rl_glue.rl_start()\n\n        best_arm = np.argmax(rl_glue.environment.arms)\n\n        scores = [0]\n        averages = []\n        best_action_chosen = []\n\n        for i in range(num_steps):\n            reward, state, action, is_terminal = rl_glue.rl_step()\n            scores.append(scores[-1] + reward)\n            averages.append(scores[-1] / (i + 1))\n            if action == best_arm:\n                best_action_chosen.append(1)\n            else:\n                best_action_chosen.append(0)\n            if epsilon == 0.1 and run == 0:\n                n_q_values.append(np.copy(rl_glue.agent.q_values))\n        if epsilon == 0.1:\n            n_averages.append(averages)\n            n_best_actions.append(best_action_chosen)\n        all_averages.append(averages)\n\n    plt.plot(np.mean(all_averages, axis=0))\n\nplt.legend([\"Best Possible\"] + epsilons)\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Average reward\")\nplt.show()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:03&lt;00:00, 55.88it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:03&lt;00:00, 57.05it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:03&lt;00:00, 59.90it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:02&lt;00:00, 67.24it/s]\n</code></pre> <p></p> <p>Why did 0.1 perform better than 0.01?</p> <p>If exploration helps why did 0.4 perform worse that 0.0 (the greedy agent)?</p> <p>Think about these and how you would answer these questions. They are questions in the practice quiz. If you still have questions about it, retake the practice quiz.</p>"},{"location":"%20Bandits%20and%20Exploration%3AExploitation%20Assignment/Assignment1/#section-4-the-effect-of-step-size","title":"Section 4: The Effect of Step Size","text":"<p>In Section 1 of this assignment, we decayed the step size over time based on action-selection counts. The step-size was 1/N(A), where N(A) is the number of times action A was selected. This is the same as computing a sample average. We could also set the step size to be a constant value, such as 0.1. What would be the effect of doing that? And is it better to use a constant or the sample average method? </p> <p>To investigate this question, let\u2019s start by creating a new agent that has a constant step size. This will be nearly identical to the agent created above. You will use the same code to select the epsilon-greedy action. You will change the update to have a constant step size instead of using the 1/N(A) update.</p> <pre><code># -----------\n# Graded Cell\n# -----------\nclass EpsilonGreedyAgentConstantStepsize(main_agent.Agent):\n    def agent_step(self, reward, observation):\n        \"\"\"\n        Takes one step for the agent. It takes in a reward and observation and \n        returns the action the agent chooses at that time step.\n\n        Arguments:\n        reward -- float, the reward the agent recieved from the environment after taking the last action.\n        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n                              until future lessons\n        Returns:\n        current_action -- int, the action chosen by the agent at the current time step.\n        \"\"\"\n\n        ### Useful Class Variables ###\n        # self.q_values : An array with what the agent believes each of the values of the arm are.\n        # self.arm_count : An array with a count of the number of times each arm has been pulled.\n        # self.last_action : An int of the action that the agent took on the previous time step.\n        # self.step_size : A float which is the current step size for the agent.\n        # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)\n        #######################\n\n        # Update q_values for action taken at previous time step \n        # using self.step_size intead of using self.arm_count\n        # your code here\n        if self.last_action is not None:\n            self.q_values[self.last_action] += self.step_size * (reward - self.q_values[self.last_action]) \n\n        # Choose action using epsilon greedy. This is the same as you implemented above.\n        # your code here\n        p = np.random.random()\n        if p &lt; self.epsilon:\n            current_action = np.random.choice(self.num_actions)\n        else:\n            current_action = argmax(self.q_values)\n        self.last_action = current_action\n\n        return current_action\n</code></pre> <pre><code># --------------\n# Debugging Cell\n# --------------\n# Feel free to make any changes to this cell to debug your code\n\nfor step_size in [0.01, 0.1, 0.5, 1.0]:\n    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()\n    e_greedy_agent.q_values = [0, 0, 1.0, 0, 0]\n    e_greedy_agent.num_actions = 5\n    e_greedy_agent.last_action = 1\n    e_greedy_agent.epsilon = 0.0\n    e_greedy_agent.step_size = step_size\n    action = e_greedy_agent.agent_step(1, 0)\n    assert e_greedy_agent.q_values == [0, step_size, 1.0, 0, 0], \"Check that you are updating q_values correctly using the stepsize.\"\n</code></pre> <pre><code># -----------\n# Tested Cell\n# -----------\n# The contents of the cell will be tested by the autograder.\n# If they do not pass here, they will not pass there.\n\nnp.random.seed(0)\n# Check Epsilon Greedy with Different Constant Stepsizes\nfor step_size in [0.01, 0.1, 0.5, 1.0]:\n    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()\n    e_greedy_agent.q_values = [0, 0, 1.0, 0, 0]\n    e_greedy_agent.num_actions = 5\n    e_greedy_agent.last_action = 1\n    e_greedy_agent.epsilon = 0.0\n    e_greedy_agent.step_size = step_size\n\n    action = e_greedy_agent.agent_step(1, 0)\n\n    assert e_greedy_agent.q_values == [0, step_size, 1.0, 0, 0]    \n</code></pre> <pre><code># ---------------\n# Discussion Cell\n# ---------------\n\n# Experiment code for different step sizes\nstep_sizes = [0.01, 0.1, 0.5, 1.0, '1/N(A)']\n\nepsilon = 0.1\nnum_steps = 1000\nnum_runs = 200\n\nfig, ax = plt.subplots(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n\nq_values = {step_size: [] for step_size in step_sizes}\ntrue_values = {step_size: None for step_size in step_sizes}\nbest_actions = {step_size: [] for step_size in step_sizes}\n\nfor step_size in step_sizes:\n    all_averages = []\n    for run in tqdm(range(num_runs)):\n        np.random.seed(run)\n        agent = EpsilonGreedyAgentConstantStepsize if step_size != '1/N(A)' else EpsilonGreedyAgent\n        agent_info = {\"num_actions\": 10, \"epsilon\": epsilon, \"step_size\": step_size, \"initial_value\": 0.0}\n        env_info = {}\n\n        rl_glue = RLGlue(env, agent)\n        rl_glue.rl_init(agent_info, env_info)\n        rl_glue.rl_start()\n\n        best_arm = np.argmax(rl_glue.environment.arms)\n\n        if run == 0:\n            true_values[step_size] = np.copy(rl_glue.environment.arms)\n\n        best_action_chosen = []\n        for i in range(num_steps):\n            reward, state, action, is_terminal = rl_glue.rl_step()\n            if action == best_arm:\n                best_action_chosen.append(1)\n            else:\n                best_action_chosen.append(0)\n            if run == 0:\n                q_values[step_size].append(np.copy(rl_glue.agent.q_values))\n        best_actions[step_size].append(best_action_chosen)\n    ax.plot(np.mean(best_actions[step_size], axis=0))\n\nplt.legend(step_sizes)\nplt.title(\"% Best Arm Pulled\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"% Best Arm Pulled\")\nvals = ax.get_yticks()\nax.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\nplt.show()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:02&lt;00:00, 72.69it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:02&lt;00:00, 71.36it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:02&lt;00:00, 70.41it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:02&lt;00:00, 72.10it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:02&lt;00:00, 68.18it/s]\n</code></pre> <p></p> <p>Notice first that we are now plotting the amount of time that the best action is taken rather than the average reward. To better  understand the performance of an agent, it can be useful to measure specific behaviors, beyond just how much reward is accumulated. This measure indicates how close the agent\u2019s behaviour is to optimal.</p> <p>It seems as though 1/N(A) performed better than the others, in that it reaches a solution where it takes the best action most frequently. Now why might this be? Why did a step size of 0.5 start out better but end up performing worse? Why did a step size of 0.01 perform so poorly?</p> <p>Let's dig into this further below. Let\u2019s plot how well each agent tracks the true value, where each agent has a different step size method. You do not have to enter any code here, just follow along.</p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\nlargest = 0\nnum_steps = 1000\nfor step_size in step_sizes:\n    plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n    largest = np.argmax(true_values[step_size])\n    plt.plot([true_values[step_size][largest] for _ in range(num_steps)], linestyle=\"--\")\n    plt.title(\"Step Size: {}\".format(step_size))\n    plt.plot(np.array(q_values[step_size])[:, largest])\n    plt.legend([\"True Expected Value\", \"Estimated Value\"])\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Value\")\n    plt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p>These plots help clarify the performance differences between the different step sizes. A step size of 0.01 makes such small updates that the agent\u2019s value estimate of the best action does not get close to the actual value. Step sizes of 0.5 and 1.0 both get close to the true value quickly, but are very susceptible to stochasticity in the rewards. The updates overcorrect too much towards recent rewards, and so oscillate around the true value. This means that on many steps, the action that pulls the best arm may seem worse than it actually is.  A step size of 0.1 updates fairly quickly to the true value, and does not oscillate as widely around the true values as 0.5 and 1.0. This is one of the reasons that 0.1 performs quite well. Finally we see why 1/N(A) performed well. Early on while the step size is still reasonably high it moves quickly to the true expected value, but as it gets pulled more its step size is reduced which makes it less susceptible to the stochasticity of the rewards.</p> <p>Does this mean that 1/N(A) is always the best? When might it not be? One possible setting where it might not be as effective is in non-stationary problems. You learned about non-stationarity in the lessons. Non-stationarity means that the environment may change over time. This could manifest itself as continual change over time of the environment, or a sudden change in the environment.</p> <p>Let's look at how a sudden change in the reward distributions affects a step size like 1/N(A). This time we will run the environment for 2000 steps, and after 1000 steps we will randomly change the expected value of all of the arms. We compare two agents, both using epsilon-greedy with epsilon = 0.1. One uses a constant step size of 0.1, the other a step size of 1/N(A) that reduces over time. </p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\nepsilon = 0.1\nnum_steps = 2000\nnum_runs = 500\nstep_size = 0.1\n\nplt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\nplt.plot([1.55 for _ in range(num_steps)], linestyle=\"--\")\n\nfor agent in [EpsilonGreedyAgent, EpsilonGreedyAgentConstantStepsize]:\n    rewards = np.zeros((num_runs, num_steps))\n    for run in tqdm(range(num_runs)):\n        agent_info = {\"num_actions\": 10, \"epsilon\": epsilon, \"step_size\": step_size}\n        np.random.seed(run)\n\n        rl_glue = RLGlue(env, agent)\n        rl_glue.rl_init(agent_info, env_info)\n        rl_glue.rl_start()\n\n        for i in range(num_steps):\n            reward, state, action, is_terminal = rl_glue.rl_step()\n            rewards[run, i] = reward\n            if i == 1000:\n                rl_glue.environment.arms = np.random.randn(10)\n\n    plt.plot(np.mean(rewards, axis=0))\nplt.legend([\"Best Possible\", \"1/N(A)\", \"0.1\"])\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Average reward\")\nplt.show()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:14&lt;00:00, 34.81it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:13&lt;00:00, 35.80it/s]\n</code></pre> <p></p> <p>Now the agent with a step size of 1/N(A) performed better at the start but then performed worse when the environment changed! What happened?</p> <p>Think about what the step size would be after 1000 steps. Let's say the best action gets chosen 500 times. That means the step size for that action is 1/500 or 0.002. At each step when we update the value of the action and the value is going to move only 0.002 * the error. That is a very tiny adjustment and it will take a long time for it to get to the true value.</p> <p>The agent with step size 0.1, however, will always update in 1/10th of the direction of the error. This means that on average it will take ten steps for it to update its value to the sample mean.</p> <p>These are the types of tradeoffs we have to think about in reinforcement learning. A larger step size moves us more quickly toward the true value, but can make our estimated values oscillate around the expected value. A step size that reduces over time can converge to close to the expected value, without oscillating. On the other hand, such a decaying stepsize is not able to adapt to changes in the environment. Nonstationarity---and the related concept of partial observability---is a common feature of reinforcement learning problems and when learning online.  </p>"},{"location":"%20Bandits%20and%20Exploration%3AExploitation%20Assignment/Assignment1/#section-5-conclusion","title":"Section 5: Conclusion","text":"<p>Great work! You have: - Implemented your first agent - Learned about the effect of epsilon, an exploration parameter, on the performance of an agent - Learned about the effect of step size on the performance of the agent - Learned about a good experiment practice of averaging across multiple runs</p>"},{"location":"Optimal%20Policies%20with%20Dynamic%20Programming%20Assignment/Assignment2/","title":"Assignment 2: Optimal Policies with Dynamic Programming","text":"<p>Welcome to Assignment 2. This notebook will help you understand: - Policy Evaluation and Policy Improvement. - Value and Policy Iteration. - Bellman Equations.</p>"},{"location":"Optimal%20Policies%20with%20Dynamic%20Programming%20Assignment/Assignment2/#gridworld-city","title":"Gridworld City","text":"<p>Gridworld City, a thriving metropolis with a booming technology industry, has recently experienced an influx of grid-loving software engineers. Unfortunately, the city's street parking system, which charges a fixed rate, is struggling to keep up with the increased demand. To address this, the city council has decided to modify the pricing scheme to better promote social welfare. In general, the city considers social welfare higher when more parking is being used, the exception being that the city prefers that at least one spot is left unoccupied (so that it is available in case someone really needs it). The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. Now the city has hired you \u2014 an expert in dynamic programming \u2014 to help determine an optimal policy.</p>"},{"location":"Optimal%20Policies%20with%20Dynamic%20Programming%20Assignment/Assignment2/#preliminaries","title":"Preliminaries","text":"<p>You'll need two imports to complete this assigment: - numpy: The fundamental package for scientific computing with Python. - tools: A module containing an environment and a plotting function.</p> <p>There are also some other lines in the cell below that are used for grading and plotting \u2014 you needn't worry about them.</p> <p>In this notebook, all cells are locked except those that you are explicitly asked to modify. It is up to you to decide how to implement your solution in these cells, but please do not import other libraries \u2014 doing so will break the autograder.</p> <pre><code>%matplotlib inline\nimport numpy as np\nimport tools\nimport grader\n</code></pre> <pre><code>&lt;Figure size 432x288 with 0 Axes&gt;\n</code></pre> <p>In the city council's parking MDP, states are nonnegative integers indicating how many parking spaces are occupied, actions are nonnegative integers designating the price of street parking, the reward is a real value describing the city's preference for the situation, and time is discretized by hour. As might be expected, charging a high price is likely to decrease occupancy over the hour, while charging a low price is likely to increase it.</p> <p>For now, let's consider an environment with three parking spaces and three price points. Note that an environment with three parking spaces actually has four states \u2014 zero, one, two, or three spaces could be occupied.</p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\nnum_spaces = 3\nnum_prices = 3\nenv = tools.ParkingWorld(num_spaces, num_prices)\nV = np.zeros(num_spaces + 1)\npi = np.ones((num_spaces + 1, num_prices)) / num_prices\n</code></pre> <p>The value function is a one-dimensional array where the $i$-th entry gives the value of $i$ spaces being occupied.</p> <pre><code>V\n\n</code></pre> <pre><code>array([0., 0., 0., 0.])\n</code></pre> <p>We can represent the policy as a two-dimensional array where the $(i, j)$-th entry gives the probability of taking action $j$ in state $i$.</p> <pre><code>pi\n</code></pre> <pre><code>array([[0.33333333, 0.33333333, 0.33333333],\n       [0.33333333, 0.33333333, 0.33333333],\n       [0.33333333, 0.33333333, 0.33333333],\n       [0.33333333, 0.33333333, 0.33333333]])\n</code></pre> <pre><code>pi[0] = [0.75, 0.11, 0.14]\n\nfor s, pi_s in enumerate(pi):\n    for a, p in enumerate(pi_s):\n        print(f'pi(A={a}|S={s}) = {p.round(2)}    ', end='')\n    print()\n</code></pre> <pre><code>pi(A=0|S=0) = 0.75    pi(A=1|S=0) = 0.11    pi(A=2|S=0) = 0.14    \npi(A=0|S=1) = 0.33    pi(A=1|S=1) = 0.33    pi(A=2|S=1) = 0.33    \npi(A=0|S=2) = 0.33    pi(A=1|S=2) = 0.33    pi(A=2|S=2) = 0.33    \npi(A=0|S=3) = 0.33    pi(A=1|S=3) = 0.33    pi(A=2|S=3) = 0.33\n</code></pre> <pre><code>V[0] = 1\n\ntools.plot(V, pi)\n</code></pre> <p></p> <p>We can visualize a value function and policy with the <code>plot</code> function in the <code>tools</code> module. On the left, the value function is displayed as a barplot. State zero has an expected return of ten, while the other states have an expected return of zero. On the right, the policy is displayed on a two-dimensional grid. Each vertical strip gives the policy at the labeled state. In state zero, action zero is the darkest because the agent's policy makes this choice with the highest probability. In the other states the agent has the equiprobable policy, so the vertical strips are colored uniformly.</p> <p>You can access the state space and the action set as attributes of the environment.</p> <pre><code>env.S\n</code></pre> <pre><code>[0, 1, 2, 3]\n</code></pre> <pre><code>env.A\n</code></pre> <pre><code>[0, 1, 2]\n</code></pre> <p>You will need to use the environment's <code>transitions</code> method to complete this assignment. The method takes a state and an action and returns a 2-dimensional array, where the entry at $(i, 0)$ is the reward for transitioning to state $i$ from the current state and the entry at $(i, 1)$ is the conditional probability of transitioning to state $i$ given the current state and action.</p> <pre><code>state = 3\naction = 1\ntransitions = env.transitions(state, action)\ntransitions\n</code></pre> <pre><code>array([[1.        , 0.12390437],\n       [2.        , 0.15133714],\n       [3.        , 0.1848436 ],\n       [2.        , 0.53991488]])\n</code></pre> <pre><code>for sp, (r, p) in enumerate(transitions):\n    print(f'p(S\\'={sp}, R={r} | S={state}, A={action}) = {p.round(2)}')\n</code></pre> <pre><code>p(S'=0, R=1.0 | S=3, A=1) = 0.12\np(S'=1, R=2.0 | S=3, A=1) = 0.15\np(S'=2, R=3.0 | S=3, A=1) = 0.18\np(S'=3, R=2.0 | S=3, A=1) = 0.54\n</code></pre>"},{"location":"Optimal%20Policies%20with%20Dynamic%20Programming%20Assignment/Assignment2/#section-1-policy-evaluation","title":"Section 1: Policy Evaluation","text":"<p>You're now ready to begin the assignment! First, the city council would like you to evaluate the quality of the existing pricing scheme. Policy evaluation works by iteratively applying the Bellman equation for $v_{\\pi}$ to a working value function, as an update rule, as shown below.</p> <p>$$\\large v(s) \\leftarrow \\sum_a \\pi(a | s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v(s')]$$ This update can either occur \"in-place\" (i.e. the update rule is sequentially applied to each state) or with \"two-arrays\" (i.e. the update rule is simultaneously applied to each state). Both versions converge to $v_{\\pi}$ but the in-place version usually converges faster. In this assignment, we will be implementing all update rules in-place, as is done in the pseudocode of chapter 4 of the textbook. </p> <p>We have written an outline of the policy evaluation algorithm described in chapter 4.1 of the textbook. It is left to you to fill in the <code>bellman_update</code> function to complete the algorithm.</p> <pre><code># lock\ndef evaluate_policy(env, V, pi, gamma, theta):\n    delta = float('inf')\n    while delta &gt; theta:\n        delta = 0\n        for s in env.S:\n            v = V[s]\n            bellman_update(env, V, pi, s, gamma)\n            delta = max(delta, abs(v - V[s]))\n\n    return V\n</code></pre> <pre><code># -----------\n# Graded Cell\n# -----------\ndef bellman_update(env, V, pi, s, gamma):\n    \"\"\"Mutate ``V`` according to the Bellman update equation.\"\"\"\n    # YOUR CODE HERE\n    action_values = []\n    for a in range(len(env.A)):\n        next_state_value = [prob * (reward + gamma * V[next_s]) for next_s, (reward,prob) in enumerate(env.transitions(s,a))]\n        action_value = pi[s][a] * sum(next_state_value)\n        action_values.append(action_value)\n    V[s] = sum(action_values)\n\n</code></pre> <p>The cell below uses the policy evaluation algorithm to evaluate the city's policy, which charges a constant price of one.</p> <pre><code># --------------\n# Debugging Cell\n# --------------\n# Feel free to make any changes to this cell to debug your code\n\n# set up test environment\nnum_spaces = 10\nnum_prices = 4\nenv = tools.ParkingWorld(num_spaces, num_prices)\n\n# build test policy\ncity_policy = np.zeros((num_spaces + 1, num_prices))\ncity_policy[:, 1] = 1\n\ngamma = 0.9\ntheta = 0.1\n\nV = np.zeros(num_spaces + 1)\nV = evaluate_policy(env, V, city_policy, gamma, theta)\n\nprint(V)\n</code></pre> <pre><code>[80.04173399 81.65532303 83.37394007 85.12975566 86.87174913 88.55589131\n 90.14020422 91.58180605 92.81929841 93.78915889 87.77792991]\n</code></pre> <pre><code># -----------\n# Tested Cell\n# -----------\n# The contents of the cell will be tested by the autograder.\n# If they do not pass here, they will not pass there.\n\n# set up test environment\nnum_spaces = 10\nnum_prices = 4\nenv = tools.ParkingWorld(num_spaces, num_prices)\n\n# build test policy\ncity_policy = np.zeros((num_spaces + 1, num_prices))\ncity_policy[:, 1] = 1\n\ngamma = 0.9\ntheta = 0.1\n\nV = np.zeros(num_spaces + 1)\nV = evaluate_policy(env, V, city_policy, gamma, theta)\n\n# test the value function\nanswer = [80.04, 81.65, 83.37, 85.12, 86.87, 88.55, 90.14, 91.58, 92.81, 93.78, 87.77]\n\n# make sure the value function is within 2 decimal places of the correct answer\nassert grader.near(V, answer, 1e-2)\n</code></pre> <p>You can use the <code>plot</code> function to visualize the final value function and policy.</p> <pre><code># lock\ntools.plot(V, city_policy)\n</code></pre> <p></p> <p>Observe that the value function qualitatively resembles the city council's preferences \u2014 it monotonically increases as more parking is used, until there is no parking left, in which case the value is lower. Because of the relatively simple reward function (more reward is accrued when many but not all parking spots are taken and less reward is accrued when few or all parking spots are taken) and the highly stochastic dynamics function (each state has positive probability of being reached each time step) the value functions of most policies will qualitatively resemble this graph. However, depending on the intelligence of the policy, the scale of the graph will differ. In other words, better policies will increase the expected return at every state rather than changing the relative desirability of the states. Intuitively, the value of a less desirable state can be increased by making it less likely to remain in a less desirable state. Similarly, the value of a more desirable state can be increased by making it more likely to remain in a more desirable state. That is to say, good policies are policies that spend more time in desirable states and less time in undesirable states. As we will see in this assignment, such a steady state distribution is achieved by setting the price to be low in low occupancy states (so that the occupancy will increase) and setting the price high when occupancy is high (so that full occupancy will be avoided).</p>"},{"location":"Optimal%20Policies%20with%20Dynamic%20Programming%20Assignment/Assignment2/#section-2-policy-iteration","title":"Section 2: Policy Iteration","text":"<p>Now the city council would like you to compute a more efficient policy using policy iteration. Policy iteration works by alternating between evaluating the existing policy and making the policy greedy with respect to the existing value function. We have written an outline of the policy iteration algorithm described in chapter 4.3 of the textbook. We will make use of the policy evaluation algorithm you completed in section 1. It is left to you to fill in the <code>q_greedify_policy</code> function, such that it modifies the policy at $s$ to be greedy with respect to the q-values at $s$, to complete the policy improvement algorithm.</p> <pre><code>def improve_policy(env, V, pi, gamma):\n    policy_stable = True\n    for s in env.S:\n        old = pi[s].copy()\n        q_greedify_policy(env, V, pi, s, gamma)\n\n        if not np.array_equal(pi[s], old):\n            policy_stable = False\n\n    return pi, policy_stable\n\ndef policy_iteration(env, gamma, theta):\n    V = np.zeros(len(env.S))\n    pi = np.ones((len(env.S), len(env.A))) / len(env.A)\n    policy_stable = False\n\n    while not policy_stable:\n        V = evaluate_policy(env, V, pi, gamma, theta)\n        pi, policy_stable = improve_policy(env, V, pi, gamma)\n\n    return V, pi\n</code></pre> <pre><code># -----------\n# Graded Cell\n# -----------\ndef q_greedify_policy(env, V, pi, s, gamma):\n    \"\"\"Mutate ``pi`` to be greedy with respect to the q-values induced by ``V``.\"\"\"\n    # YOUR CODE HERE\n    action_values = {}\n    for a in range(len(env.A)):\n        action_value = [prob * (reward + gamma * V[next_s]) for next_s, (reward, prob) in enumerate(env.transitions(s,a))]\n        action_values[a] = sum(action_value)\n    max_qvalue_action = max(action_values,key=action_values.get)\n    for a in range(len(env.A)):\n        if a == max_qvalue_action:\n            pi[s][a] = 1\n        else:\n            pi[s][a] = 0\n\n\n</code></pre> <pre><code># --------------\n# Debugging Cell\n# --------------\n# Feel free to make any changes to this cell to debug your code\n\ngamma = 0.9\ntheta = 0.1\nenv = tools.ParkingWorld(num_spaces=6, num_prices=4)\n\nV = np.array([7, 6, 5, 4, 3, 2, 1])\npi = np.ones((7, 4)) / 4\n\nnew_pi, stable = improve_policy(env, V, pi, gamma)\n\n# expect first call to greedify policy\nexpected_pi = np.array([\n    [0, 0, 0, 1],\n    [0, 0, 0, 1],\n    [0, 0, 0, 1],\n    [0, 0, 0, 1],\n    [0, 0, 0, 1],\n    [0, 0, 0, 1],\n    [0, 0, 0, 1],\n])\nassert np.all(new_pi == expected_pi)\nassert stable == False\n\n# the value function has not changed, so the greedy policy should not change\nnew_pi, stable = improve_policy(env, V, new_pi, gamma)\n\nassert np.all(new_pi == expected_pi)\nassert stable == True\n</code></pre> <pre><code># -----------\n# Tested Cell\n# -----------\n# The contents of the cell will be tested by the autograder.\n# If they do not pass here, they will not pass there.\ngamma = 0.9\ntheta = 0.1\nenv = tools.ParkingWorld(num_spaces=10, num_prices=4)\n\nV, pi = policy_iteration(env, gamma, theta)\n\nV_answer = [81.60, 83.28, 85.03, 86.79, 88.51, 90.16, 91.70, 93.08, 94.25, 95.25, 89.45]\npi_answer = [\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [0, 0, 0, 1],\n    [0, 0, 0, 1],\n]\n\n# make sure value function is within 2 decimal places of answer\nassert grader.near(V, V_answer, 1e-2)\n\n# make sure policy is exactly correct\nassert np.all(pi == pi_answer)\n</code></pre> <p>When you are ready to test the policy iteration algorithm, run the cell below.</p> <pre><code>env = tools.ParkingWorld(num_spaces=10, num_prices=4)\ngamma = 0.9\ntheta = 0.1\nV, pi = policy_iteration(env, gamma, theta)\n</code></pre> <p>You can use the <code>plot</code> function to visualize the final value function and policy.</p> <pre><code>tools.plot(V, pi)\n</code></pre> <p></p> <p>You can check the value function (rounded to one decimal place) and policy against the answer below: State $\\quad\\quad$    Value $\\quad\\quad$ Action 0 $\\quad\\quad\\quad\\;$        81.6 $\\quad\\quad\\;$ 0 1 $\\quad\\quad\\quad\\;$        83.3 $\\quad\\quad\\;$ 0 2 $\\quad\\quad\\quad\\;$        85.0 $\\quad\\quad\\;$ 0 3 $\\quad\\quad\\quad\\;$        86.8 $\\quad\\quad\\;$ 0 4 $\\quad\\quad\\quad\\;$        88.5 $\\quad\\quad\\;$ 0 5 $\\quad\\quad\\quad\\;$        90.2 $\\quad\\quad\\;$ 0 6 $\\quad\\quad\\quad\\;$        91.7 $\\quad\\quad\\;$ 0 7 $\\quad\\quad\\quad\\;$        93.1 $\\quad\\quad\\;$ 0 8 $\\quad\\quad\\quad\\;$        94.3 $\\quad\\quad\\;$ 0 9 $\\quad\\quad\\quad\\;$        95.3 $\\quad\\quad\\;$ 3 10 $\\quad\\quad\\;\\;\\,\\,$      89.5 $\\quad\\quad\\;$ 3</p>"},{"location":"Optimal%20Policies%20with%20Dynamic%20Programming%20Assignment/Assignment2/#section-3-value-iteration","title":"Section 3: Value Iteration","text":"<p>The city has also heard about value iteration and would like you to implement it. Value iteration works by iteratively applying the Bellman optimality equation for $v_{\\ast}$ to a working value function, as an update rule, as shown below.</p> <p>$$\\large v(s) \\leftarrow \\max_a \\sum_{s', r} p(s', r | s, a)[r + \\gamma v(s')]$$ We have written an outline of the value iteration algorithm described in chapter 4.4 of the textbook. It is left to you to fill in the <code>bellman_optimality_update</code> function to complete the value iteration algorithm.</p> <pre><code>def value_iteration(env, gamma, theta):\n    V = np.zeros(len(env.S))\n    while True:\n        delta = 0\n        for s in env.S:\n            v = V[s]\n            bellman_optimality_update(env, V, s, gamma)\n            delta = max(delta, abs(v - V[s]))\n        if delta &lt; theta:\n            break\n    pi = np.ones((len(env.S), len(env.A))) / len(env.A)\n    for s in env.S:\n        q_greedify_policy(env, V, pi, s, gamma)\n    return V, pi\n</code></pre> <pre><code># -----------\n# Graded Cell\n# -----------\ndef bellman_optimality_update(env, V, s, gamma):\n    \"\"\"Mutate ``V`` according to the Bellman optimality update equation.\"\"\"\n    # YOUR CODE HERE\n    # \u7406\u89e3\u6210\u7528bellman\u6700\u4f18\u516c\u5f0f\u66f4\u65b0\u5c31\u597d\u4e86\n    # \u9009\u62e9\u4e00\u4e2a\u6700\u4f18a\n    action_values = []\n    for a in range(len(env.A)):\n        next_state_value = [prob * (reward + gamma * V[next_s]) for next_s, (reward, prob) in enumerate(env.transitions(s,a))]\n        action_value = sum(next_state_value)\n        action_values.append(action_value)\n    V[s] = max(action_values)\n\n</code></pre> <pre><code># --------------\n# Debugging Cell\n# --------------\n# Feel free to make any changes to this cell to debug your code\n\ngamma = 0.9\nenv = tools.ParkingWorld(num_spaces=6, num_prices=4)\n\nV = np.array([7, 6, 5, 4, 3, 2, 1])\n\n# only state 0 updated\nbellman_optimality_update(env, V, 0, gamma)\nassert list(V) == [5, 6, 5, 4, 3, 2, 1]\n\n# only state 2 updated\nbellman_optimality_update(env, V, 2, gamma)\nassert list(V) == [5, 6, 7, 4, 3, 2, 1]\n</code></pre> <pre><code># -----------\n# Tested Cell\n# -----------\n# The contents of the cell will be tested by the autograder.\n# If they do not pass here, they will not pass there.\ngamma = 0.9\nenv = tools.ParkingWorld(num_spaces=10, num_prices=4)\n\nV = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n\nfor _ in range(10):\n    for s in env.S:\n        bellman_optimality_update(env, V, s, gamma)\n\n# make sure value function is exactly correct\nanswer = [61, 63, 65, 67, 69, 71, 72, 74, 75, 76, 71]\nassert np.all(V == answer)\n</code></pre> <p>When you are ready to test the value iteration algorithm, run the cell below.</p> <pre><code>env = tools.ParkingWorld(num_spaces=10, num_prices=4)\ngamma = 0.9\ntheta = 0.1\nV, pi = value_iteration(env, gamma, theta)\n</code></pre> <p>You can use the <code>plot</code> function to visualize the final value function and policy.</p> <pre><code>tools.plot(V, pi)\n</code></pre> <p></p> <p>You can check your value function (rounded to one decimal place) and policy against the answer below: State $\\quad\\quad$    Value $\\quad\\quad$ Action 0 $\\quad\\quad\\quad\\;$        81.6 $\\quad\\quad\\;$ 0 1 $\\quad\\quad\\quad\\;$        83.3 $\\quad\\quad\\;$ 0 2 $\\quad\\quad\\quad\\;$        85.0 $\\quad\\quad\\;$ 0 3 $\\quad\\quad\\quad\\;$        86.8 $\\quad\\quad\\;$ 0 4 $\\quad\\quad\\quad\\;$        88.5 $\\quad\\quad\\;$ 0 5 $\\quad\\quad\\quad\\;$        90.2 $\\quad\\quad\\;$ 0 6 $\\quad\\quad\\quad\\;$        91.7 $\\quad\\quad\\;$ 0 7 $\\quad\\quad\\quad\\;$        93.1 $\\quad\\quad\\;$ 0 8 $\\quad\\quad\\quad\\;$        94.3 $\\quad\\quad\\;$ 0 9 $\\quad\\quad\\quad\\;$        95.3 $\\quad\\quad\\;$ 3 10 $\\quad\\quad\\;\\;\\,\\,$      89.5 $\\quad\\quad\\;$ 3</p> <p>In the value iteration algorithm above, a policy is not explicitly maintained until the value function has converged. Below, we have written an identically behaving value iteration algorithm that maintains an updated policy. Writing value iteration in this form makes its relationship to policy iteration more evident. Policy iteration alternates between doing complete greedifications and complete evaluations. On the other hand, value iteration alternates between doing local greedifications and local evaluations. </p> <pre><code>def value_iteration2(env, gamma, theta):\n    V = np.zeros(len(env.S))\n    pi = np.ones((len(env.S), len(env.A))) / len(env.A)\n    while True:\n        delta = 0\n        for s in env.S:\n            v = V[s]\n            q_greedify_policy(env, V, pi, s, gamma)\n            bellman_update(env, V, pi, s, gamma)\n            delta = max(delta, abs(v - V[s]))\n        if delta &lt; theta:\n            break\n    return V, pi\n</code></pre> <p>You can try the second value iteration algorithm by running the cell below.</p> <pre><code>env = tools.ParkingWorld(num_spaces=10, num_prices=4)\ngamma = 0.9\ntheta = 0.1\nV, pi = value_iteration2(env, gamma, theta)\ntools.plot(V, pi)\n</code></pre> <p></p>"},{"location":"Optimal%20Policies%20with%20Dynamic%20Programming%20Assignment/Assignment2/#wrapping-up","title":"Wrapping Up","text":"<p>Congratulations, you've completed assignment 2! In this assignment, we investigated policy evaluation and policy improvement, policy iteration and value iteration, and Bellman updates. Gridworld City thanks you for your service!</p>"},{"location":"Q-learning%20and%20Expected%20Sarsa%20Assignment/assignment/","title":"Assignment 2 - Q-Learning and Expected Sarsa","text":"<p>Welcome to Course 2 Programming Assignment 2. In this notebook, you will:</p> <ol> <li>Implement Q-Learning with $\\epsilon$-greedy action selection</li> <li>Implement Expected Sarsa with $\\epsilon$-greedy action selection</li> <li>Investigate how these two algorithms behave on Cliff World (described on page 132 of the textbook)</li> </ol> <p>We will provide you with the environment and infrastructure to run an experiment (called the experiment program in RL-Glue). This notebook will provide all the code you need to run your experiment and visualise learning performance.</p> <p>This assignment will be graded automatically by comparing the behavior of your agent to our implementations of Expected Sarsa and Q-learning. The random seed will be set to avoid different behavior due to randomness. We will highlight the functions you have to use for generating random samples and the number of times these functions should be called. </p>"},{"location":"Q-learning%20and%20Expected%20Sarsa%20Assignment/assignment/#packages","title":"Packages","text":"<p>You will need the following libraries for this assignment. We are using: 1. numpy: the fundamental package for scientific computing with Python. 2. scipy: a Python library for scientific and technical computing. 3. matplotlib: library for plotting graphs in Python. 4. RL-Glue: library for reinforcement learning experiments.</p> <p>DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</p> <pre><code>%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom scipy.stats import sem\n\nfrom rl_glue import RLGlue\nfrom agent import BaseAgent\nimport cliffworld_env\n</code></pre> <pre><code>plt.rcParams.update({'font.size': 15})\nplt.rcParams.update({'figure.figsize': [10,5]})\n</code></pre>"},{"location":"Q-learning%20and%20Expected%20Sarsa%20Assignment/assignment/#q-learning","title":"Q-Learning","text":"<p>In this section you will implement and test a Q-Learning agent with $\\epsilon$-greedy action selection (Section 6.5 in the textbook). </p>"},{"location":"Q-learning%20and%20Expected%20Sarsa%20Assignment/assignment/#implementation","title":"Implementation","text":"<p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p> <pre><code># -----------\n# Graded Cell\n# -----------\n\nclass QLearningAgent(BaseAgent):\n    def agent_init(self, agent_init_info):\n        \"\"\"Setup for the agent called when the experiment first starts.\n\n        Args:\n        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n        {\n            num_states (int): The number of states,\n            num_actions (int): The number of actions,\n            epsilon (float): The epsilon parameter for exploration,\n            step_size (float): The step-size,\n            discount (float): The discount factor,\n        }\n\n        \"\"\"\n        # Store the parameters provided in agent_init_info.\n        self.num_actions = agent_init_info[\"num_actions\"]\n        self.num_states = agent_init_info[\"num_states\"]\n        self.epsilon = agent_init_info[\"epsilon\"]\n        self.step_size = agent_init_info[\"step_size\"]\n        self.discount = agent_init_info[\"discount\"]\n        self.rand_generator = np.random.RandomState(agent_info[\"seed\"])\n\n        # Create an array for action-value estimates and initialize it to zero.\n        self.q = np.zeros((self.num_states, self.num_actions)) # The array of action-value estimates.\n        # the mapping of the states and actions\n\n\n    def agent_start(self, observation):\n        \"\"\"The first method called when the episode starts, called after\n        the environment starts.\n        Args:\n            observation (int): the state observation from the\n                environment's evn_start function.\n        Returns:\n            action (int): the first action the agent takes.\n        \"\"\"\n\n        # Choose action using epsilon greedy.\n        state = observation\n        current_q = self.q[state,:] # state(and mapping four actions)\n        if self.rand_generator.rand() &lt; self.epsilon:\n            action = self.rand_generator.randint(self.num_actions)\n        else:\n            action = self.argmax(current_q)\n        self.prev_state = state\n        self.prev_action = action\n        return action\n\n    def agent_step(self, reward, observation):\n        \"\"\"A step taken by the agent.\n        Args:\n            reward (float): the reward received for taking the last action taken\n            observation (int): the state observation from the\n                environment's step based on where the agent ended up after the\n                last step.w\n        Returns:\n            action (int): the action the agent is taking.\n        \"\"\"\n\n        # Choose action using epsilon greedy.\n        state = observation\n        current_q = self.q[state, :]\n        if self.rand_generator.rand() &lt; self.epsilon:\n            action = self.rand_generator.randint(self.num_actions)\n        else:\n            action = self.argmax(current_q)\n\n        # Perform an update\n        # --------------------------\n        # your code here\n        target = reward + self.discount * np.max(self.q[state, :])\n        self.q[self.prev_state, self.prev_action] = self.q[self.prev_state, self.prev_action] + self.step_size * (target - self.q[self.prev_state, self.prev_action])\n        # --------------------------\n\n        self.prev_state = state\n        self.prev_action = action\n        return action\n\n    def agent_end(self, reward):\n        \"\"\"Run when the agent terminates.\n        Args:\n            reward (float): the reward the agent received for entering the\n                terminal state.\n        \"\"\"\n        # Perform the last update in the episode\n        # --------------------------\n        # your code here\n        target = reward\n        self.q[self.prev_state, self.prev_action] += self.step_size * (target - self.q[self.prev_state, self.prev_action])\n        # --------------------------\n\n    def argmax(self, q_values):\n        \"\"\"argmax with random tie-breaking\n        Args:\n            q_values (Numpy array): the array of action-values\n        Returns:\n            action (int): an action with the highest value\n        \"\"\"\n        top = float(\"-inf\")\n        ties = []\n\n        for i in range(len(q_values)):\n            if q_values[i] &gt; top:\n                top = q_values[i]\n                ties = []\n\n            if q_values[i] == top:\n                ties.append(i)\n\n        return self.rand_generator.choice(ties)\n</code></pre>"},{"location":"Q-learning%20and%20Expected%20Sarsa%20Assignment/assignment/#test","title":"Test","text":"<p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p> <p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p> <pre><code># -----------\n# Tested Cell\n# -----------\n# The contents of the cell will be tested by the autograder.\n# If they do not pass here, they will not pass there.\n\nnp.random.seed(0)\n\nagent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\nagent = QLearningAgent()\nagent.agent_init(agent_info)\naction = agent.agent_start(0)\nexpected_values = np.array([\n    [0, 0, 0, 0],\n    [0, 0, 0, 0],\n    [0, 0, 0, 0],\n])\n\nassert np.all(agent.q == expected_values)\nassert action == 1\n\n# reset the agent\nagent.agent_init(agent_info)\n\naction = agent.agent_start(0)\n\nassert action == 1\n\naction = agent.agent_step(2, 1)\nassert action == 3\n\naction = agent.agent_step(0, 0)\nassert action == 1\n\nexpected_values = np.array([\n    [0.,  0.2,  0.,  0.  ],\n    [0.,  0.,   0.,  0.02],\n    [0.,  0.,   0.,  0.  ],\n])\nassert np.all(np.isclose(agent.q, expected_values))\n\n# reset the agent\nagent.agent_init(agent_info)\n\naction = agent.agent_start(0)\nassert action == 1\n\naction = agent.agent_step(2, 1)\nassert action == 3\n\nagent.agent_end(1)\n\nexpected_values = np.array([\n    [0.,  0.2, 0.,  0. ],\n    [0.,  0.,  0.,  0.1],\n    [0.,  0.,  0.,  0. ],\n])\nassert np.all(np.isclose(agent.q, expected_values))\n\n# Run a few more tests to ensure the epsilon-random action is not picked in the update\nexpected_values = np.array([\n    [0.,         0.2,        0.,         0.        ],\n    [5.97824336, 5.75000715, 5.79372928, 6.69483878],\n    [0.,         0.,         0.,         0.        ],\n])\nagent.epsilon = 1.0  # Set epsilon high so that there is a larger chance to catch the errors\nfor _ in range(100):\n    agent.agent_step(2, 1)\nassert np.all(np.isclose(agent.q, expected_values))\n</code></pre>"},{"location":"Q-learning%20and%20Expected%20Sarsa%20Assignment/assignment/#expected-sarsa","title":"Expected Sarsa","text":"<p>In this section you will implement an Expected Sarsa agent with $\\epsilon$-greedy action selection (Section 6.6 in the textbook). </p>"},{"location":"Q-learning%20and%20Expected%20Sarsa%20Assignment/assignment/#implementation_1","title":"Implementation","text":"<p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p> <pre><code># -----------\n# Graded Cell\n# -----------\n\nclass ExpectedSarsaAgent(BaseAgent):\n    def agent_init(self, agent_init_info):\n        \"\"\"Setup for the agent called when the experiment first starts.\n\n        Args:\n        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n        {\n            num_states (int): The number of states,\n            num_actions (int): The number of actions,\n            epsilon (float): The epsilon parameter for exploration,\n            step_size (float): The step-size,\n            discount (float): The discount factor,\n        }\n\n        \"\"\"\n        # Store the parameters provided in agent_init_info.\n        self.num_actions = agent_init_info[\"num_actions\"]\n        self.num_states = agent_init_info[\"num_states\"]\n        self.epsilon = agent_init_info[\"epsilon\"]\n        self.step_size = agent_init_info[\"step_size\"]\n        self.discount = agent_init_info[\"discount\"]\n        self.rand_generator = np.random.RandomState(agent_info[\"seed\"])\n\n        # Create an array for action-value estimates and initialize it to zero.\n        self.q = np.zeros((self.num_states, self.num_actions)) # The array of action-value estimates.\n\n\n    def agent_start(self, observation):\n        \"\"\"The first method called when the episode starts, called after\n        the environment starts.\n        Args:\n            observation (int): the state observation from the\n                environment's evn_start function.\n        Returns:\n            action (int): the first action the agent takes.\n        \"\"\"\n\n        # Choose action using epsilon greedy.\n        state = observation\n        current_q = self.q[state, :]\n        # epsilon-greedy algorithm\n        if self.rand_generator.rand() &lt; self.epsilon:\n            action = self.rand_generator.randint(self.num_actions)\n        else:\n            action = self.argmax(current_q)\n        self.prev_state = state\n        self.prev_action = action\n        return action\n\n    def agent_step(self, reward, observation):\n        \"\"\"A step taken by the agent.\n        Args:\n            reward (float): the reward received for taking the last action taken\n            observation (int): the state observation from the\n                environment's step based on where the agent ended up after the\n                last step.\n        Returns:\n            action (int): the action the agent is taking.\n        \"\"\"\n\n        # Choose action using epsilon greedy.\n        state = observation\n        current_q = self.q[state,:]\n        if self.rand_generator.rand() &lt; self.epsilon:\n            action = self.rand_generator.randint(self.num_actions)\n        else:\n            action = self.argmax(current_q)\n\n        # Perform an update\n        # --------------------------\n        # your code here\n        next_q = self.q[state, :]\n        expected_value = np.sum(next_q * self.epsilon / self.num_actions) + np.sum(next_q * (1 - self.epsilon) * np.identity(self.num_actions)[np.argmax(next_q), :])\n        target = reward + self.discount*expected_value\n\n        self.q[self.prev_state, self.prev_action] += self.step_size * (target - self.q[self.prev_state, self.prev_action])\n        # --------------------------\n\n        self.prev_state = state\n        self.prev_action = action\n        return action\n\n    def agent_end(self, reward):\n        \"\"\"Run when the agent terminates.\n        Args:\n            reward (float): the reward the agent received for entering the\n                terminal state.\n        \"\"\"\n        # Perform the last update in the episode\n        # --------------------------\n        # your code here\n        target = reward\n        self.q[self.prev_state, self.prev_action] += self.step_size * (target - self.q[self.prev_state, self.prev_action])\n        # --------------------------\n\n    def argmax(self, q_values):\n        \"\"\"argmax with random tie-breaking\n        Args:\n            q_values (Numpy array): the array of action-values\n        Returns:\n            action (int): an action with the highest value\n        \"\"\"\n        top = float(\"-inf\")\n        ties = []\n\n        for i in range(len(q_values)):\n            if q_values[i] &gt; top:\n                top = q_values[i]\n                ties = []\n\n            if q_values[i] == top:\n                ties.append(i)\n\n        return self.rand_generator.choice(ties)\n</code></pre>"},{"location":"Q-learning%20and%20Expected%20Sarsa%20Assignment/assignment/#test_1","title":"Test","text":"<p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p> <p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p> <pre><code># -----------\n# Tested Cell\n# -----------\n# The contents of the cell will be tested by the autograder.\n# If they do not pass here, they will not pass there.\n\nagent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\nagent = ExpectedSarsaAgent()\nagent.agent_init(agent_info)\n\naction = agent.agent_start(0)\nassert action == 1\n\nexpected_values = np.array([\n    [0, 0, 0, 0],\n    [0, 0, 0, 0],\n    [0, 0, 0, 0],\n])\nassert np.all(agent.q == expected_values)\n\n# ---------------\n# test agent step\n# ---------------\n\naction = agent.agent_step(2, 1)\nassert action == 3\n\naction = agent.agent_step(0, 0)\nassert action == 1\n\nexpected_values = np.array([\n    [0, 0.2, 0, 0],\n    [0, 0, 0, 0.0185],\n    [0, 0, 0, 0],\n])\nassert np.all(np.isclose(agent.q, expected_values))\n\n# --------------\n# test agent end\n# --------------\n\nagent.agent_end(1)\n\nexpected_values = np.array([\n    [0, 0.28, 0, 0],\n    [0, 0, 0, 0.0185],\n    [0, 0, 0, 0],\n])\nassert np.all(np.isclose(agent.q, expected_values))\n</code></pre>"},{"location":"Q-learning%20and%20Expected%20Sarsa%20Assignment/assignment/#solving-the-cliff-world","title":"Solving the Cliff World","text":"<p>We described the Cliff World environment in the video \"Expected Sarsa in the Cliff World\" in Lesson 3. This is an undiscounted episodic task and thus we set $\\gamma$=1. The agent starts in the bottom left corner of the gridworld below and takes actions that move it in the four directions. Actions that would move the agent off of the cliff incur a reward of -100 and send the agent back to the start state. The reward for all other transitions is -1. An episode terminates when the agent reaches the bottom right corner. </p> <p></p> <p>Using the experiment program in the cell below we now compare the agents on the Cliff World environment and plot the sum of rewards during each episode for the two agents.</p> <p>The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\nnp.random.seed(0)\n\nagents = {\n    \"Q-learning\": QLearningAgent,\n    \"Expected Sarsa\": ExpectedSarsaAgent\n}\nenv = cliffworld_env.Environment\nall_reward_sums = {} # Contains sum of rewards during episode\nall_state_visits = {} # Contains state visit counts during the last 10 episodes\nagent_info = {\"num_actions\": 4, \"num_states\": 48, \"epsilon\": 0.1, \"step_size\": 0.5, \"discount\": 1.0}\nenv_info = {}\nnum_runs = 100 # The number of runs\nnum_episodes = 200 # The number of episodes in each run\n\nfor algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n    all_reward_sums[algorithm] = []\n    all_state_visits[algorithm] = []\n    for run in tqdm(range(num_runs)):\n        agent_info[\"seed\"] = run\n        rl_glue = RLGlue(env, agents[algorithm])\n        rl_glue.rl_init(agent_info, env_info)\n\n        reward_sums = []\n        state_visits = np.zeros(48)\n        for episode in range(num_episodes):\n            if episode &lt; num_episodes - 10:\n                # Runs an episode\n                rl_glue.rl_episode(10000) \n            else: \n                # Runs an episode while keeping track of visited states\n                state, action = rl_glue.rl_start()\n                state_visits[state] += 1\n                is_terminal = False\n                while not is_terminal:\n                    reward, state, action, is_terminal = rl_glue.rl_step()\n                    state_visits[state] += 1\n\n            reward_sums.append(rl_glue.rl_return())\n\n        all_reward_sums[algorithm].append(reward_sums)\n        all_state_visits[algorithm].append(state_visits)\n\n# plot results\nfor algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n    plt.plot(np.mean(all_reward_sums[algorithm], axis=0), label=algorithm)\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Sum of\\n rewards\\n during\\n episode\",rotation=0, labelpad=40)\nplt.ylim(-100,0)\nplt.legend()\nplt.show()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:19&lt;00:00,  5.26it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:31&lt;00:00,  3.22it/s]\n</code></pre> <p></p> <p>To see why these two agents behave differently, let's inspect the states they visit most. Run the cell below to generate plots showing the number of timesteps that the agents spent in each state over the last 10 episodes.</p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\n\nfor algorithm, position in [(\"Q-learning\", 211), (\"Expected Sarsa\", 212)]:\n    plt.subplot(position)\n    average_state_visits = np.array(all_state_visits[algorithm]).mean(axis=0)\n    grid_state_visits = average_state_visits.reshape((4,12))\n    grid_state_visits[0,1:-1] = np.nan\n    plt.pcolormesh(grid_state_visits, edgecolors='gray', linewidth=2)\n    plt.title(algorithm)\n    plt.axis('off')\n    cm = plt.get_cmap()\n    cm.set_bad('gray')\n\n    plt.subplots_adjust(bottom=0.0, right=0.7, top=1.0)\n    cax = plt.axes([0.85, 0.0, 0.075, 1.])\n\ncbar = plt.colorbar(cax=cax)\ncbar.ax.set_ylabel(\"Visits during\\n the last 10\\n episodes\", rotation=0, labelpad=70)\nplt.show()\n</code></pre> <p></p> <p>The Q-learning agent learns the optimal policy, one that moves along the cliff and reaches the goal in as few steps as possible. However, since the agent does not follow the optimal policy and uses $\\epsilon$-greedy exploration, it occasionally falls off the cliff. The Expected Sarsa agent takes exploration into account and follows a safer path.</p> <p>Previously we used a fixed step-size of 0.5 for the agents. What happens with other step-sizes? Does this difference in performance persist?</p> <p>In the next experiment we will try 10 different step-sizes from 0.1 to 1.0 and compare the sum of rewards per episode averaged over the first 100 episodes (similar to the interim performance curves in Figure 6.3 of the textbook). Shaded regions show standard errors.</p> <p>This cell takes around 10 minutes to run. The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p> <pre><code># ---------------\n# Discussion Cell\n# ---------------\nfrom itertools import product\n\nagents = {\n    \"Q-learning\": QLearningAgent,\n    \"Expected Sarsa\": ExpectedSarsaAgent\n}\nenv = cliffworld_env.Environment\nall_reward_sums = {}\nstep_sizes = np.linspace(0.1,1.0,10)\nagent_info = {\"num_actions\": 4, \"num_states\": 48, \"epsilon\": 0.1, \"discount\": 1.0}\nenv_info = {}\nnum_runs = 30\nnum_episodes = 100\nall_reward_sums = {}\n\nalgorithms = [\"Q-learning\", \"Expected Sarsa\"]\ncross_product = list(product(algorithms, step_sizes, range(num_runs)))\nfor algorithm, step_size, run in tqdm(cross_product):\n    if (algorithm, step_size) not in all_reward_sums:\n        all_reward_sums[(algorithm, step_size)] = []\n\n    agent_info[\"step_size\"] = step_size\n    agent_info[\"seed\"] = run\n    rl_glue = RLGlue(env, agents[algorithm])\n    rl_glue.rl_init(agent_info, env_info)\n\n    last_episode_total_reward = 0\n    for episode in range(num_episodes):\n        rl_glue.rl_episode(0)\n    all_reward_sums[(algorithm, step_size)].append(rl_glue.rl_return()/num_episodes)\n\n\nfor algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm, step_size)]) for step_size in step_sizes])\n    algorithm_stds = np.array([sem(all_reward_sums[(algorithm, step_size)]) for step_size in step_sizes])\n    plt.plot(step_sizes, algorithm_means, marker='o', linestyle='solid', label=algorithm)\n    plt.fill_between(step_sizes, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=0.2)\n\nplt.legend()\nplt.xlabel(\"Step-size\")\nplt.ylabel(\"Sum of\\n rewards\\n per episode\",rotation=0, labelpad=50)\nplt.xticks(step_sizes)\nplt.show()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 600/600 [02:01&lt;00:00,  4.94it/s]\n</code></pre> <p></p> <p>Expected Sarsa shows an advantage over Q-learning in this problem across a wide range of step-sizes.</p> <p>Congratulations! Now you have:</p> <ul> <li>implemented Q-Learning with $\\epsilon$-greedy action selection</li> <li>implemented Expected Sarsa with $\\epsilon$-greedy action selection</li> <li>investigated the behavior of these two algorithms on Cliff World</li> </ul>"}]}